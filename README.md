# mccviahat

Minimal working setup for:
- Dockerized LLM server (FastAPI + Transformers) on CPU
- Designed to be reproducible on fresh CloudLab nodes
