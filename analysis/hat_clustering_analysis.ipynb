{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAT Clustering Analysis — Emotional vs Neutral (Per-Prompt Isolated)\n",
    "\n",
    "Each data point is one permutation run — a full inference over a concatenated sequence of stories,\n",
    "isolated via server reset + cache drop before each trial.\n",
    "\n",
    "**Data layout per data point** (`runs/<label>/p<NNNN>/`):\n",
    "```\n",
    "perf_stat.csv        — 1ms buckets, 24 perf events (HAT Layer 1 tracepoints + Layer 2 PMU/power)\n",
    "hat_interrupts.csv   — 100ms: /proc/interrupts, /proc/softirqs, CPU freq per core\n",
    "proc_sample.csv      — 100ms: system + per-process CPU jiffies, RSS\n",
    "collector_meta.json  — t0_ns, duration, perf_events list\n",
    "trial_meta.json      — prompt_index, label, ok, t_request_start_ns, t_request_end_ns, elapsed_ms\n",
    "```\n",
    "\n",
    "**Experimental design:**\n",
    "- 120 emotional permutations (5! orderings of 5 ptsd stories)\n",
    "- 120 neutral permutations (5! orderings of 5 WikiHow articles)\n",
    "- Neutral prompts are shorter on average → `elapsed_ms` used as a confounder proxy for prompt length\n",
    "- Each trial is fully isolated: server reset + `drop_caches` + 2s stabilisation baseline\n",
    "\n",
    "**Analysis approach:**\n",
    "1. Extract 10 indicator metrics per HAT indicator (event-specific or PCI-specific, as appropriate)\n",
    "2. Build a feature matrix: 240 rows × N metrics\n",
    "3. Per-metric univariate screening (Mann-Whitney U + effect size)\n",
    "4. Per-metric k=2 clustering → silhouette score → rank metrics by separability\n",
    "5. Multi-metric GMM clustering on top-K features\n",
    "6. Confounder check: elapsed_ms (prompt duration proxy for length difference)\n",
    "7. PCoA / UMAP visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import json, re, warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import scipy.stats as sp_stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 — Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral trials:   120\n",
      "Emotional trials: 120\n"
     ]
    }
   ],
   "source": [
    "# ── Set this to your mccviahat root ──\n",
    "BASE_DIR = Path.home() / 'Desktop' / 'mccviahat'\n",
    "\n",
    "NEUTRAL_ROOT   = BASE_DIR / 'runs' / 'neutral' \n",
    "EMOTIONAL_ROOT = BASE_DIR / 'runs' / 'emotional'\n",
    "\n",
    "for d in [NEUTRAL_ROOT, EMOTIONAL_ROOT]:\n",
    "    assert d.exists(), f'Missing run root: {d}'\n",
    "\n",
    "n_trials = sorted(NEUTRAL_ROOT.glob('p????'))\n",
    "e_trials = sorted(EMOTIONAL_ROOT.glob('p????'))\n",
    "\n",
    "print(f'Neutral trials:   {len(n_trials)}')\n",
    "print(f'Emotional trials: {len(e_trials)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 — Loaders\n",
    "\n",
    "Each data point is a directory. We load:\n",
    "- `perf_stat.csv` (wide format, 1ms resolution) — event-indicators + PCIs from perf\n",
    "- `hat_interrupts.csv` (100ms) — cumulative /proc/interrupts counters, converted to deltas\n",
    "- `trial_meta.json` — elapsed_ms (used as confounder for prompt length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     p \u001b[38;5;241m=\u001b[39m trial_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollector_meta.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(p\u001b[38;5;241m.\u001b[39mread_text()) \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_perf\u001b[39m(trial_dir: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m:\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load perf_stat.csv (wide: t_s, event1, ...). Falls back to parsing perf_stat.txt.\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     csv_p \u001b[38;5;241m=\u001b[39m trial_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperf_stat.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "def load_trial_meta(trial_dir: Path) -> dict:\n",
    "    p = trial_dir / 'trial_meta.json'\n",
    "    return json.loads(p.read_text()) if p.exists() else {}\n",
    "\n",
    "\n",
    "def load_collector_meta(trial_dir: Path) -> dict:\n",
    "    p = trial_dir / 'collector_meta.json'\n",
    "    return json.loads(p.read_text()) if p.exists() else {}\n",
    "\n",
    "\n",
    "def load_perf(trial_dir: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"Load perf_stat.csv (wide: t_s, event1, ...). Falls back to parsing perf_stat.txt.\"\"\"\n",
    "    csv_p = trial_dir / 'perf_stat.csv'\n",
    "    txt_p = trial_dir / 'perf_stat.txt'\n",
    "    if csv_p.exists() and csv_p.stat().st_size > 0:\n",
    "        return pd.read_csv(csv_p)\n",
    "    if txt_p.exists() and txt_p.stat().st_size > 0:\n",
    "        return _parse_perf_txt(txt_p)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_perf_txt(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse perf stat -x ',' output into wide DataFrame.\"\"\"\n",
    "    from collections import OrderedDict\n",
    "    rows_by_ts: dict = OrderedDict()\n",
    "    events_seen: list = []\n",
    "    for line in path.read_text(encoding='utf-8', errors='replace').splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "        parts = line.split(',')\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        try:\n",
    "            ts = float(parts[0])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        event = parts[3].strip()\n",
    "        if not event:\n",
    "            continue\n",
    "        val_s = parts[1].strip()\n",
    "        val = float('nan') if (val_s.startswith('<') or val_s == '') else float(val_s) if val_s.replace('.','',1).isdigit() else float('nan')\n",
    "        if event not in events_seen:\n",
    "            events_seen.append(event)\n",
    "        rows_by_ts.setdefault(ts, {})[event] = val\n",
    "    records = [{'t_s': ts, **evts} for ts, evts in rows_by_ts.items()]\n",
    "    df = pd.DataFrame(records).sort_values('t_s').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_hat_interrupts(trial_dir: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"Load hat_interrupts.csv with cumulative counters converted to per-sample deltas.\"\"\"\n",
    "    p = trial_dir / 'hat_interrupts.csv'\n",
    "    if not p.exists() or p.stat().st_size == 0:\n",
    "        return None\n",
    "    df = pd.read_csv(p)\n",
    "    df['t_s'] = (df['timestamp_ns'] - df['timestamp_ns'].iloc[0]) / 1e9\n",
    "    # Convert cumulative /proc/interrupts counts to deltas (rate per sample)\n",
    "    irq_cols = [c for c in df.columns\n",
    "                if c not in ('timestamp_ns', 't_s')\n",
    "                and not c.endswith('_freq_khz')\n",
    "                and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    for c in irq_cols:\n",
    "        df[c] = df[c].diff().clip(lower=0)  # deltas; first row becomes NaN → drop later\n",
    "    df = df.iloc[1:].reset_index(drop=True)  # drop first NaN row\n",
    "    return df\n",
    "\n",
    "\n",
    "print('Loaders defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 — Metric definitions\n",
    "\n",
    "We compute **10 metrics** per indicator, split by indicator type:\n",
    "\n",
    "| # | Metric | Applies to | Description |\n",
    "|---|--------|------------|-------------|\n",
    "| 1 | `mean_rate` | both | Mean count/value per time unit |\n",
    "| 2 | `variance` | both (PCIs esp.) | Spread around the mean |\n",
    "| 3 | `p90_p10` | both | Robust range (P90 − P10) |\n",
    "| 4 | `slope` | PCIs esp. | Linear trend (drift during inference) |\n",
    "| 5 | `spectral_entropy` | PCIs esp. | PSD entropy — structured vs broadband |\n",
    "| 6 | `iat_cv` | event-indicators | Coefficient of variation of inter-arrival times |\n",
    "| 7 | `burst_rate` | event-indicators | Bursts (runs > mean+1σ) per second |\n",
    "| 8 | `burst_clustering` | event-indicators | Fraction of events inside bursts |\n",
    "| 9 | `lz_complexity` | both | Lempel-Ziv complexity (binarised) |\n",
    "| 10 | `perm_entropy` | both | Normalised permutation entropy (order=3) |\n",
    "\n",
    "**Neutral prompts are shorter** → elapsed_ms differs systematically. We record it as a confounder\n",
    "and use it to check that any detected separation is not purely a length artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Metric helpers ───────────────────────────────────────────────────────────\n",
    "\n",
    "def _safe(series: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drop NaN/Inf, return float64.\"\"\"\n",
    "    s = np.asarray(series, dtype=float)\n",
    "    return s[np.isfinite(s)]\n",
    "\n",
    "\n",
    "def metric_mean_rate(s: np.ndarray, dt_s: float) -> float:\n",
    "    \"\"\"Mean count per second (event-indicators) or mean value (PCIs).\"\"\"\n",
    "    s = _safe(s)\n",
    "    return float(s.mean() / dt_s) if (len(s) > 0 and dt_s > 0) else np.nan\n",
    "\n",
    "\n",
    "def metric_variance(s: np.ndarray) -> float:\n",
    "    s = _safe(s)\n",
    "    return float(np.var(s, ddof=1)) if len(s) > 1 else np.nan\n",
    "\n",
    "\n",
    "def metric_p90_p10(s: np.ndarray) -> float:\n",
    "    s = _safe(s)\n",
    "    return float(np.percentile(s, 90) - np.percentile(s, 10)) if len(s) > 1 else np.nan\n",
    "\n",
    "\n",
    "def metric_slope(s: np.ndarray) -> float:\n",
    "    \"\"\"OLS slope (value per sample).\"\"\"\n",
    "    s = _safe(s)\n",
    "    if len(s) < 3:\n",
    "        return np.nan\n",
    "    x = np.arange(len(s), dtype=float)\n",
    "    return float(np.polyfit(x, s, 1)[0])\n",
    "\n",
    "\n",
    "def metric_spectral_entropy(s: np.ndarray) -> float:\n",
    "    \"\"\"Normalised Shannon entropy of power spectral density.\"\"\"\n",
    "    s = _safe(s)\n",
    "    if len(s) < 4:\n",
    "        return np.nan\n",
    "    psd = np.abs(np.fft.rfft(s - s.mean())) ** 2\n",
    "    psd = psd[1:]  # skip DC\n",
    "    if psd.sum() == 0:\n",
    "        return 0.0\n",
    "    p = psd / psd.sum()\n",
    "    h = -np.sum(p * np.log2(p + 1e-15))\n",
    "    return float(h / np.log2(len(p))) if len(p) > 1 else 0.0\n",
    "\n",
    "\n",
    "def metric_iat_cv(s: np.ndarray) -> float:\n",
    "    \"\"\"CV of inter-arrival times (only meaningful for event-indicators).\"\"\"\n",
    "    s = _safe(s)\n",
    "    arrivals = np.where(s > 0)[0]\n",
    "    if len(arrivals) < 3:\n",
    "        return np.nan\n",
    "    iat = np.diff(arrivals).astype(float)\n",
    "    mu = iat.mean()\n",
    "    return float(iat.std(ddof=1) / mu) if mu > 0 else np.nan\n",
    "\n",
    "\n",
    "def metric_burst_rate(s: np.ndarray, dur_s: float) -> float:\n",
    "    \"\"\"Bursts (contiguous runs above mean+1σ) per second.\"\"\"\n",
    "    s = _safe(s)\n",
    "    if len(s) < 2 or s.std() == 0:\n",
    "        return 0.0\n",
    "    thresh = s.mean() + s.std()\n",
    "    above = (s > thresh).astype(int)\n",
    "    diff = np.diff(np.concatenate(([0], above, [0])))\n",
    "    n_bursts = int((diff == 1).sum())\n",
    "    return float(n_bursts / dur_s) if dur_s > 0 else 0.0\n",
    "\n",
    "\n",
    "def metric_burst_clustering(s: np.ndarray) -> float:\n",
    "    \"\"\"Fraction of total events that fall inside bursts.\"\"\"\n",
    "    s = _safe(s)\n",
    "    if len(s) < 2 or s.std() == 0 or s.sum() == 0:\n",
    "        return 0.0\n",
    "    thresh = s.mean() + s.std()\n",
    "    above = (s > thresh).astype(int)\n",
    "    diff = np.diff(np.concatenate(([0], above, [0])))\n",
    "    starts = np.where(diff == 1)[0]\n",
    "    ends   = np.where(diff == -1)[0]\n",
    "    if len(starts) == 0:\n",
    "        return 0.0\n",
    "    events_in_bursts = sum(s[st:en].sum() for st, en in zip(starts, ends))\n",
    "    return float(events_in_bursts / s.sum())\n",
    "\n",
    "\n",
    "def metric_lz_complexity(s: np.ndarray) -> float:\n",
    "    \"\"\"LZ76 complexity of binarised (> median) series, normalised.\"\"\"\n",
    "    s = _safe(s)\n",
    "    if len(s) < 4:\n",
    "        return np.nan\n",
    "    binary = (s > np.median(s)).astype(int)\n",
    "    seq = ''.join(map(str, binary))\n",
    "    n = len(seq)\n",
    "    i, k, l, c = 0, 1, 1, 1\n",
    "    while k + l <= n:\n",
    "        if seq[i + l - 1] == seq[k + l - 1]:\n",
    "            l += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            if i == k:\n",
    "                c += 1; k += l; i = 0; l = 1\n",
    "            else:\n",
    "                l = 1\n",
    "    c += 1\n",
    "    return float(c / (n / np.log2(n))) if n > 1 else 0.0\n",
    "\n",
    "\n",
    "def metric_perm_entropy(s: np.ndarray, order: int = 3) -> float:\n",
    "    \"\"\"Normalised permutation entropy.\"\"\"\n",
    "    import math\n",
    "    s = _safe(s)\n",
    "    if len(s) < order:\n",
    "        return np.nan\n",
    "    counts = defaultdict(int)\n",
    "    for i in range(len(s) - order + 1):\n",
    "        counts[tuple(np.argsort(s[i:i+order]))] += 1\n",
    "    total = sum(counts.values())\n",
    "    probs = np.array(list(counts.values())) / total\n",
    "    h = -np.sum(probs * np.log2(probs + 1e-15))\n",
    "    h_max = np.log2(math.factorial(order))\n",
    "    return float(h / h_max) if h_max > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_all_metrics(s: np.ndarray, dur_s: float, indicator_type: str = 'event') -> dict:\n",
    "    \"\"\"\n",
    "    Compute all 10 metrics for one indicator series.\n",
    "    indicator_type: 'event' (discrete, irregular) or 'pci' (periodic, continuous).\n",
    "    Both types get all 10 metrics; IAT-based ones return NaN for PCIs (constant interval).\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'mean_rate':        metric_mean_rate(s, dur_s),\n",
    "        'variance':         metric_variance(s),\n",
    "        'p90_p10':          metric_p90_p10(s),\n",
    "        'slope':            metric_slope(s),\n",
    "        'spectral_entropy': metric_spectral_entropy(s),\n",
    "        'iat_cv':           metric_iat_cv(s) if indicator_type == 'event' else np.nan,\n",
    "        'burst_rate':       metric_burst_rate(s, dur_s),\n",
    "        'burst_clustering': metric_burst_clustering(s),\n",
    "        'lz_complexity':    metric_lz_complexity(s),\n",
    "        'perm_entropy':     metric_perm_entropy(s),\n",
    "    }\n",
    "\n",
    "\n",
    "print('Metric functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 — Feature extraction\n",
    "\n",
    "Loop over all trials, load their perf and HAT interrupt data, compute metrics for each indicator.\n",
    "One row = one trial = one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify perf events by indicator type\n",
    "EVENT_INDICATORS = {\n",
    "    'irq:irq_handler_entry', 'irq:irq_handler_exit',\n",
    "    'irq:softirq_entry', 'irq:softirq_exit', 'irq:softirq_raise',\n",
    "    'tlb:tlb_flush', 'mce:mce_record',\n",
    "    'context-switches', 'cpu-migrations', 'page-faults',\n",
    "}\n",
    "# Everything else in perf is a PCI (counter read at fixed 1ms intervals)\n",
    "\n",
    "\n",
    "def extract_trial_features(trial_dir: Path, label: str) -> dict | None:\n",
    "    \"\"\"Return a flat feature dict for one trial directory, or None on failure.\"\"\"\n",
    "    t_meta = load_trial_meta(trial_dir)\n",
    "    if not t_meta.get('ok', False):\n",
    "        return None  # failed trial\n",
    "\n",
    "    perf = load_perf(trial_dir)\n",
    "    if perf is None or len(perf) < 10:\n",
    "        return None  # collector didn't produce usable data\n",
    "\n",
    "    c_meta = load_collector_meta(trial_dir)\n",
    "    dur_s = t_meta.get('elapsed_ms', np.nan) / 1000.0\n",
    "\n",
    "    row = {\n",
    "        'condition':   label,\n",
    "        'prompt_index': t_meta.get('prompt_index', -1),\n",
    "        'elapsed_ms':  t_meta.get('elapsed_ms', np.nan),\n",
    "        'dur_s':       dur_s,\n",
    "    }\n",
    "\n",
    "    # ── perf indicators ──\n",
    "    perf_events = [c for c in perf.columns if c != 't_s']\n",
    "    for evt in perf_events:\n",
    "        s = perf[evt].values.astype(float)\n",
    "        itype = 'event' if evt in EVENT_INDICATORS else 'pci'\n",
    "        metrics = compute_all_metrics(s, dur_s, itype)\n",
    "        for m_name, m_val in metrics.items():\n",
    "            row[f'{evt}__{m_name}'] = m_val\n",
    "\n",
    "    # ── HAT interrupt indicators (delta counts at 100ms) ──\n",
    "    hat = load_hat_interrupts(trial_dir)\n",
    "    if hat is not None:\n",
    "        hat_irq_cols = [c for c in hat.columns\n",
    "                        if c not in ('timestamp_ns', 't_s')\n",
    "                        and not c.endswith('_freq_khz')\n",
    "                        and pd.api.types.is_numeric_dtype(hat[c])]\n",
    "        hat_freq_cols = [c for c in hat.columns if c.endswith('_freq_khz')]\n",
    "\n",
    "        for col in hat_irq_cols:\n",
    "            s = hat[col].values.astype(float)\n",
    "            metrics = compute_all_metrics(s, dur_s, 'event')\n",
    "            for m_name, m_val in metrics.items():\n",
    "                row[f'hat_{col}__{m_name}'] = m_val\n",
    "\n",
    "        # CPU frequency: PCI at 100ms — mean and variance per trial\n",
    "        if hat_freq_cols:\n",
    "            freq_mean = hat[hat_freq_cols].mean(axis=1).values\n",
    "            metrics = compute_all_metrics(freq_mean, dur_s, 'pci')\n",
    "            for m_name, m_val in metrics.items():\n",
    "                row[f'cpu_freq_mean__{m_name}'] = m_val\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "# ── Run extraction ──\n",
    "print('Extracting features…')\n",
    "records = []\n",
    "skipped = 0\n",
    "\n",
    "for trial_dir in n_trials:\n",
    "    r = extract_trial_features(trial_dir, 'neutral')\n",
    "    if r is not None:\n",
    "        records.append(r)\n",
    "    else:\n",
    "        skipped += 1\n",
    "\n",
    "for trial_dir in e_trials:\n",
    "    r = extract_trial_features(trial_dir, 'emotional')\n",
    "    if r is not None:\n",
    "        records.append(r)\n",
    "    else:\n",
    "        skipped += 1\n",
    "\n",
    "df_all = pd.DataFrame(records)\n",
    "print(f'Loaded: {len(df_all)} trials ({skipped} skipped)')\n",
    "print(f'  neutral:   {(df_all.condition == \"neutral\").sum()}')\n",
    "print(f'  emotional: {(df_all.condition == \"emotional\").sum()}')\n",
    "print(f'Feature columns: {len(df_all.columns) - 4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 — Confounder check: elapsed_ms\n",
    "\n",
    "Neutral prompts are shorter. If elapsed_ms differs significantly between conditions,\n",
    "any metric that scales with prompt duration is a confounder, not a true signal.\n",
    "We check this explicitly and will flag length-sensitive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = df_all[df_all.condition == 'neutral']\n",
    "e_rows = df_all[df_all.condition == 'emotional']\n",
    "\n",
    "print('=== elapsed_ms (proxy for prompt length) ===')\n",
    "print(f'  Neutral:   mean={n_rows.elapsed_ms.mean():.0f}ms  std={n_rows.elapsed_ms.std():.0f}ms')\n",
    "print(f'  Emotional: mean={e_rows.elapsed_ms.mean():.0f}ms  std={e_rows.elapsed_ms.std():.0f}ms')\n",
    "stat, p_len = sp_stats.mannwhitneyu(n_rows.elapsed_ms, e_rows.elapsed_ms, alternative='two-sided')\n",
    "print(f'  Mann-Whitney U p = {p_len:.4f}', '← LENGTH DIFFERS SIGNIFICANTLY' if p_len < 0.05 else '(OK, balanced)')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for ax, rows, label, color in [\n",
    "    (axes[0], n_rows, 'Neutral',   'steelblue'),\n",
    "    (axes[1], e_rows, 'Emotional', 'firebrick'),\n",
    "]:\n",
    "    ax.hist(rows.elapsed_ms / 1000, bins=20, color=color, alpha=0.8)\n",
    "    ax.set_xlabel('Trial duration (s)')\n",
    "    ax.set_title(f'{label} — trial duration')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "LENGTH_CONFOUNDED = p_len < 0.05\n",
    "print(f'\\nLength confound flag: {LENGTH_CONFOUNDED}')\n",
    "print('Note: mean_rate is normalised by dur_s, so it is NOT confounded by length.')\n",
    "print('Metrics like burst_rate (per-second) are also duration-normalised.')\n",
    "print('Metrics like variance, p90_p10, lz_complexity may scale with duration → flag if sig.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 — Build feature matrix and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_COLS = {'condition', 'prompt_index', 'elapsed_ms', 'dur_s'}\n",
    "feat_cols = [c for c in df_all.columns if c not in META_COLS]\n",
    "\n",
    "X_raw = df_all[feat_cols].copy()\n",
    "y = (df_all['condition'] == 'emotional').astype(int).values  # 0=neutral, 1=emotional\n",
    "\n",
    "# Drop zero-variance columns and fill NaN\n",
    "X_raw = X_raw.loc[:, X_raw.std() > 0]\n",
    "X_raw = X_raw.fillna(X_raw.median())\n",
    "\n",
    "feat_cols = list(X_raw.columns)\n",
    "print(f'Feature matrix: {X_raw.shape[0]} samples × {X_raw.shape[1]} features')\n",
    "\n",
    "# Standardise\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X_raw), columns=feat_cols)\n",
    "print('Standardised.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 — Univariate screening: Mann-Whitney U per metric\n",
    "\n",
    "Non-parametric test for each feature. We report effect size (rank-biserial r) and direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "n_vals = X_raw[y == 0]\n",
    "e_vals = X_raw[y == 1]\n",
    "\n",
    "for col in feat_cols:\n",
    "    nv = n_vals[col].dropna().values\n",
    "    ev = e_vals[col].dropna().values\n",
    "    if len(nv) < 5 or len(ev) < 5:\n",
    "        continue\n",
    "    if nv.std() == 0 and ev.std() == 0:\n",
    "        continue\n",
    "    stat, p = sp_stats.mannwhitneyu(nv, ev, alternative='two-sided')\n",
    "    r = 1 - (2 * stat) / (len(nv) * len(ev))  # rank-biserial r\n",
    "    results.append({\n",
    "        'feature':   col,\n",
    "        'U':         stat,\n",
    "        'p':         p,\n",
    "        'effect_r':  r,\n",
    "        'n_mean':    nv.mean(),\n",
    "        'e_mean':    ev.mean(),\n",
    "        'direction': '↑E' if ev.mean() > nv.mean() else '↓E',\n",
    "    })\n",
    "\n",
    "mwu_df = pd.DataFrame(results).sort_values('p').reset_index(drop=True)\n",
    "\n",
    "# Bonferroni-corrected threshold\n",
    "alpha_raw = 0.05\n",
    "alpha_bonf = alpha_raw / len(mwu_df)\n",
    "sig_strict = mwu_df[mwu_df.p < alpha_bonf]\n",
    "sig_nominal = mwu_df[mwu_df.p < alpha_raw]\n",
    "\n",
    "print(f'Features tested: {len(mwu_df)}')\n",
    "print(f'Significant at p<{alpha_raw} (nominal):    {len(sig_nominal)}')\n",
    "print(f'Significant at p<{alpha_bonf:.2e} (Bonferroni): {len(sig_strict)}')\n",
    "print()\n",
    "print('=== Top 30 features by p-value ===')\n",
    "print(mwu_df.head(30)[['feature', 'U', 'p', 'effect_r', 'direction']].to_string(\n",
    "    index=False, float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volcano plot: effect size vs -log10(p)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['firebrick' if r > 0 else 'steelblue' for r in mwu_df.effect_r]\n",
    "ax.scatter(-np.log10(mwu_df.p + 1e-15), mwu_df.effect_r.abs(),\n",
    "           c=colors, alpha=0.5, s=15)\n",
    "ax.axhline(0.3, ls='--', color='gray', lw=0.8, label='|r|=0.3 (medium effect)')\n",
    "ax.axvline(-np.log10(alpha_raw), ls=':', color='orange', lw=0.8, label=f'p={alpha_raw}')\n",
    "ax.axvline(-np.log10(alpha_bonf), ls=':', color='red', lw=0.8, label=f'p={alpha_bonf:.1e} (Bonf.)')\n",
    "# Label top 10\n",
    "for _, row in mwu_df.head(10).iterrows():\n",
    "    ax.annotate(row.feature.split('__')[-1] + '\\n' + row.feature.split('__')[0][:15],\n",
    "                xy=(-np.log10(row.p + 1e-15), abs(row.effect_r)),\n",
    "                fontsize=6, ha='left', va='bottom')\n",
    "ax.set_xlabel('-log₁₀(p)')\n",
    "ax.set_ylabel('|rank-biserial r|')\n",
    "ax.set_title('Volcano plot — all features (red=↑Emotional, blue=↑Neutral)')\n",
    "ax.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_volcano.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 — Per-metric k=2 clustering → silhouette ranking\n",
    "\n",
    "For each feature independently, fit k-means with k=2 and score how well the\n",
    "resulting clusters align with the ground-truth labels.\n",
    "\n",
    "Two scores per feature:\n",
    "- **Silhouette**: geometric cluster quality (−1 to 1, higher = more separated)\n",
    "- **ARI** (Adjusted Rand Index): overlap with true labels (0=random, 1=perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results = []\n",
    "\n",
    "for col in feat_cols:\n",
    "    x1d = X[[col]].values\n",
    "    if np.isnan(x1d).any():\n",
    "        continue\n",
    "    km = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "    labels_pred = km.fit_predict(x1d)\n",
    "    sil = silhouette_score(x1d, labels_pred)\n",
    "    ari = adjusted_rand_score(y, labels_pred)\n",
    "    # Cluster purity: fraction of majority label in each cluster\n",
    "    mask0 = labels_pred == 0\n",
    "    mask1 = labels_pred == 1\n",
    "    purity = max(\n",
    "        (y[mask0] == 0).mean() if mask0.sum() > 0 else 0,\n",
    "        (y[mask0] == 1).mean() if mask0.sum() > 0 else 0,\n",
    "    ) * 0.5 + max(\n",
    "        (y[mask1] == 0).mean() if mask1.sum() > 0 else 0,\n",
    "        (y[mask1] == 1).mean() if mask1.sum() > 0 else 0,\n",
    "    ) * 0.5\n",
    "    cluster_results.append({\n",
    "        'feature':   col,\n",
    "        'silhouette': sil,\n",
    "        'ari':       ari,\n",
    "        'purity':    purity,\n",
    "    })\n",
    "\n",
    "clust_df = pd.DataFrame(cluster_results).sort_values('silhouette', ascending=False).reset_index(drop=True)\n",
    "print('=== Top 30 features by silhouette score ===')\n",
    "print(clust_df.head(30).to_string(index=False, float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine MWU p-value and silhouette into one ranking table\n",
    "ranking = mwu_df[['feature', 'p', 'effect_r', 'direction']].merge(\n",
    "    clust_df[['feature', 'silhouette', 'ari', 'purity']], on='feature', how='inner')\n",
    "\n",
    "# Composite score: (1 - normalised_p_rank) * 0.5 + silhouette_rank * 0.5\n",
    "ranking['p_rank']   = ranking['p'].rank()\n",
    "ranking['sil_rank'] = (-ranking['silhouette']).rank()  # lower is better\n",
    "ranking['composite_rank'] = ranking[['p_rank', 'sil_rank']].mean(axis=1)\n",
    "ranking = ranking.sort_values('composite_rank').reset_index(drop=True)\n",
    "\n",
    "print('=== Combined ranking (MWU + silhouette) — top 30 ===')\n",
    "print(ranking.head(30)[['feature','p','effect_r','direction','silhouette','ari','purity']]\n",
    "      .to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "# Save full ranking\n",
    "ranking.to_csv(BASE_DIR / 'metric_ranking.csv', index=False)\n",
    "print(f'\\nSaved → {BASE_DIR}/metric_ranking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: top 20 features by silhouette\n",
    "top20 = clust_df.head(20)\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "colors = ['firebrick' if row.ari > 0 else 'steelblue'\n",
    "          for _, row in top20.iterrows()]\n",
    "ax.barh(range(len(top20)), top20.silhouette, color=colors, alpha=0.8)\n",
    "ax.set_yticks(range(len(top20)))\n",
    "ax.set_yticklabels(top20.feature, fontsize=8)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Silhouette score (k=2)')\n",
    "ax.set_title('Top 20 features by cluster separability\\n(red=ARI>0 → correct orientation, blue=inverted)')\n",
    "ax.axvline(0, color='black', lw=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_silhouette_ranking.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 — Per-indicator metric summary\n",
    "\n",
    "Which metric type (mean_rate, variance, burst_clustering, …) drives the most separable features?\n",
    "Aggregate silhouette scores by metric suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_NAMES = ['mean_rate','variance','p90_p10','slope','spectral_entropy',\n",
    "                'iat_cv','burst_rate','burst_clustering','lz_complexity','perm_entropy']\n",
    "\n",
    "metric_agg = []\n",
    "for m in METRIC_NAMES:\n",
    "    rows = clust_df[clust_df.feature.str.endswith(f'__{m}')]\n",
    "    if len(rows) == 0:\n",
    "        continue\n",
    "    metric_agg.append({\n",
    "        'metric':       m,\n",
    "        'n_features':   len(rows),\n",
    "        'mean_sil':     rows.silhouette.mean(),\n",
    "        'max_sil':      rows.silhouette.max(),\n",
    "        'mean_ari':     rows.ari.mean(),\n",
    "        'best_feature': rows.iloc[0].feature,\n",
    "    })\n",
    "\n",
    "metric_agg_df = pd.DataFrame(metric_agg).sort_values('mean_sil', ascending=False)\n",
    "print('=== Metric type summary ===')\n",
    "print(metric_agg_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.bar(metric_agg_df.metric, metric_agg_df.mean_sil, alpha=0.8, color='steelblue', label='mean sil')\n",
    "ax.bar(metric_agg_df.metric, metric_agg_df.max_sil, alpha=0.4, color='firebrick', label='max sil')\n",
    "ax.set_ylabel('Silhouette score')\n",
    "ax.set_title('Mean / Max silhouette by metric type (across all indicators)')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_metric_type_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 — GMM clustering on top-K features\n",
    "\n",
    "Select the top K features by composite rank, fit a 2-component GMM, and evaluate\n",
    "cluster assignment against ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 20  # adjust after inspecting ranking\n",
    "\n",
    "top_features = ranking.head(TOP_K)['feature'].tolist()\n",
    "X_top = X[top_features].values\n",
    "\n",
    "# GMM with k=2\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42, n_init=10)\n",
    "gmm.fit(X_top)\n",
    "gmm_labels = gmm.predict(X_top)\n",
    "gmm_probs  = gmm.predict_proba(X_top)\n",
    "\n",
    "ari_gmm  = adjusted_rand_score(y, gmm_labels)\n",
    "sil_gmm  = silhouette_score(X_top, gmm_labels)\n",
    "\n",
    "# Align GMM label 0/1 to neutral/emotional\n",
    "# (GMM labels are arbitrary — check which assignment gives better accuracy)\n",
    "acc_direct  = (gmm_labels == y).mean()\n",
    "acc_flipped = (1 - gmm_labels == y).mean()\n",
    "if acc_flipped > acc_direct:\n",
    "    gmm_labels = 1 - gmm_labels\n",
    "    gmm_probs  = gmm_probs[:, ::-1]\n",
    "accuracy = (gmm_labels == y).mean()\n",
    "\n",
    "print(f'GMM (k=2) on top {TOP_K} features:')\n",
    "print(f'  ARI:      {ari_gmm:.4f}')\n",
    "print(f'  Silhouette: {sil_gmm:.4f}')\n",
    "print(f'  Accuracy vs ground truth: {accuracy:.1%}')\n",
    "print(f'  Neutral correctly assigned:   {(gmm_labels[y==0] == 0).mean():.1%}')\n",
    "print(f'  Emotional correctly assigned: {(gmm_labels[y==1] == 1).mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep K from 1 to 50: how does clustering quality scale with number of features?\n",
    "K_vals = list(range(1, min(51, len(feat_cols) + 1)))\n",
    "ari_vals, sil_vals, acc_vals = [], [], []\n",
    "\n",
    "for k in K_vals:\n",
    "    feats = ranking.head(k)['feature'].tolist()\n",
    "    Xk = X[feats].values\n",
    "    try:\n",
    "        gmm_k = GaussianMixture(n_components=2, covariance_type='full', random_state=42, n_init=5)\n",
    "        lbl = gmm_k.fit_predict(Xk)\n",
    "        ari_k = adjusted_rand_score(y, lbl)\n",
    "        sil_k = silhouette_score(Xk, lbl) if len(np.unique(lbl)) > 1 else 0.0\n",
    "        acc_k = max((lbl == y).mean(), (1 - lbl == y).mean())\n",
    "        ari_vals.append(ari_k); sil_vals.append(sil_k); acc_vals.append(acc_k)\n",
    "    except Exception:\n",
    "        ari_vals.append(np.nan); sil_vals.append(np.nan); acc_vals.append(np.nan)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, vals, title, color in [\n",
    "    (axes[0], ari_vals,  'ARI vs K',        'steelblue'),\n",
    "    (axes[1], sil_vals,  'Silhouette vs K', 'forestgreen'),\n",
    "    (axes[2], acc_vals,  'Accuracy vs K',   'firebrick'),\n",
    "]:\n",
    "    ax.plot(K_vals, vals, color=color, lw=1.5)\n",
    "    ax.axhline(0, color='gray', lw=0.5, ls='--')\n",
    "    ax.set_xlabel('Number of top features (K)')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.suptitle('GMM k=2 clustering quality vs number of features', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_k_sweep.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 — PCA visualisation of cluster structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X[top_features].values)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: colour by GMM cluster assignment\n",
    "ax = axes[0]\n",
    "colors_pred = ['steelblue' if l == 0 else 'firebrick' for l in gmm_labels]\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=colors_pred, alpha=0.6, s=20)\n",
    "ax.set_title('PCA — coloured by GMM cluster (blue=cluster0, red=cluster1)')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "\n",
    "# Right: colour by ground truth\n",
    "ax = axes[1]\n",
    "gt_colors = ['steelblue' if yi == 0 else 'firebrick' for yi in y]\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=gt_colors, alpha=0.6, s=20)\n",
    "ax.set_title('PCA — coloured by ground truth (blue=neutral, red=emotional)')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "\n",
    "# Custom legends\n",
    "from matplotlib.patches import Patch\n",
    "axes[0].legend(handles=[Patch(color='steelblue', label='Cluster 0'),\n",
    "                         Patch(color='firebrick', label='Cluster 1')], fontsize=8)\n",
    "axes[1].legend(handles=[Patch(color='steelblue', label='Neutral'),\n",
    "                         Patch(color='firebrick', label='Emotional')], fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_pca_clusters.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'PCA explained variance: PC1={pca.explained_variance_ratio_[0]:.1%}, PC2={pca.explained_variance_ratio_[1]:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 — Feature correlation matrix (top features)\n",
    "\n",
    "Identify redundant features. Correlated features (|r|>0.9) carry overlapping information\n",
    "and should be pruned to one representative per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_TOP = min(30, len(top_features))\n",
    "corr = X[ranking.head(CORR_TOP)['feature'].tolist()].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(corr.values, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_xticks(range(CORR_TOP))\n",
    "ax.set_yticks(range(CORR_TOP))\n",
    "ax.set_xticklabels(corr.columns, rotation=90, fontsize=6)\n",
    "ax.set_yticklabels(corr.index, fontsize=6)\n",
    "ax.set_title(f'Correlation matrix — top {CORR_TOP} features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_correlation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Report highly correlated pairs\n",
    "high_corr = []\n",
    "cols = list(corr.columns)\n",
    "for i, a in enumerate(cols):\n",
    "    for j, b in enumerate(cols):\n",
    "        if j <= i:\n",
    "            continue\n",
    "        r = corr.loc[a, b]\n",
    "        if abs(r) > 0.9:\n",
    "            high_corr.append({'feature_A': a, 'feature_B': b, 'r': r})\n",
    "if high_corr:\n",
    "    print(f'Highly correlated pairs (|r|>0.9): {len(high_corr)}')\n",
    "    print(pd.DataFrame(high_corr).to_string(index=False, float_format='{:.3f}'.format))\n",
    "else:\n",
    "    print('No highly correlated pairs among top features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 — Per-indicator: which metrics drive each indicator?\n",
    "\n",
    "For each HAT indicator, show which of the 10 metrics has the best silhouette.\n",
    "This tells you not just *that* an indicator differs, but *how* it differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse feature names: indicator__metric\n",
    "def parse_feature(fname):\n",
    "    parts = fname.rsplit('__', 1)\n",
    "    if len(parts) == 2:\n",
    "        return parts[0], parts[1]\n",
    "    return fname, ''\n",
    "\n",
    "clust_df['indicator'] = clust_df.feature.apply(lambda f: parse_feature(f)[0])\n",
    "clust_df['metric']    = clust_df.feature.apply(lambda f: parse_feature(f)[1])\n",
    "\n",
    "# Best metric per indicator\n",
    "best_per_ind = (clust_df\n",
    "    .sort_values('silhouette', ascending=False)\n",
    "    .groupby('indicator')\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    [['indicator', 'metric', 'silhouette', 'ari']]\n",
    "    .sort_values('silhouette', ascending=False)\n",
    ")\n",
    "\n",
    "print('=== Best metric per indicator (top 30 by silhouette) ===')\n",
    "print(best_per_ind.head(30).to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "# Heatmap: indicators × metrics, silhouette value\n",
    "top_inds = best_per_ind.head(15)['indicator'].tolist()\n",
    "pivot = clust_df[clust_df.indicator.isin(top_inds)].pivot_table(\n",
    "    index='indicator', columns='metric', values='silhouette', aggfunc='first')\n",
    "pivot = pivot.reindex(index=top_inds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "im = ax.imshow(pivot.values, cmap='RdYlGn', vmin=-0.5, vmax=0.5, aspect='auto')\n",
    "plt.colorbar(im, ax=ax, label='Silhouette')\n",
    "ax.set_xticks(range(len(pivot.columns)))\n",
    "ax.set_xticklabels(pivot.columns, rotation=40, ha='right', fontsize=8)\n",
    "ax.set_yticks(range(len(pivot.index)))\n",
    "ax.set_yticklabels([s[:40] for s in pivot.index], fontsize=7)\n",
    "ax.set_title('Silhouette score: top 15 indicators × 10 metrics')\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_indicator_metric_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 — Final: single-metric distribution plots for top features\n",
    "\n",
    "For the top 6 features by composite rank, show overlapping KDEs for\n",
    "neutral vs emotional — the visual confirmation of cluster separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "TOP_PLOT = 6\n",
    "plot_features = ranking.head(TOP_PLOT)['feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feat in enumerate(plot_features):\n",
    "    ax = axes[i]\n",
    "    n_vals_f = X_raw[feat][y == 0].dropna().values\n",
    "    e_vals_f = X_raw[feat][y == 1].dropna().values\n",
    "\n",
    "    for vals, label, color in [\n",
    "        (n_vals_f, 'Neutral',   'steelblue'),\n",
    "        (e_vals_f, 'Emotional', 'firebrick'),\n",
    "    ]:\n",
    "        if len(vals) < 3:\n",
    "            continue\n",
    "        kde = gaussian_kde(vals, bw_method='scott')\n",
    "        x_range = np.linspace(vals.min(), vals.max(), 200)\n",
    "        ax.fill_between(x_range, kde(x_range), alpha=0.35, color=color, label=label)\n",
    "        ax.plot(x_range, kde(x_range), color=color, lw=1.5)\n",
    "\n",
    "    # Get stats from ranking\n",
    "    row = ranking[ranking.feature == feat].iloc[0]\n",
    "    ax.set_title(f'{feat}\\np={row.p:.3e}  r={row.effect_r:.3f}  sil={row.silhouette:.3f}',\n",
    "                 fontsize=7)\n",
    "    ax.set_xlabel('Value', fontsize=7)\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('Top feature distributions: Neutral vs Emotional', fontsize=12, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_top_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 — Summary table\n",
    "\n",
    "The final output: which features survive both univariate screening and clustering,\n",
    "ranked by composite score. These are the indicators to carry forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ranking.copy()\n",
    "summary['indicator'] = summary.feature.apply(lambda f: parse_feature(f)[0])\n",
    "summary['metric']    = summary.feature.apply(lambda f: parse_feature(f)[1])\n",
    "\n",
    "# Flag if length-confounded: metrics NOT normalised by duration\n",
    "non_normalised = ['variance', 'p90_p10', 'lz_complexity', 'perm_entropy']\n",
    "summary['length_sensitive'] = summary.metric.isin(non_normalised) & LENGTH_CONFOUNDED\n",
    "\n",
    "final_cols = ['composite_rank','indicator','metric','p','effect_r','direction',\n",
    "              'silhouette','ari','purity','length_sensitive']\n",
    "print('=== Final metric ranking (top 40) ===')\n",
    "print(summary.head(40)[final_cols].to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "summary[final_cols].to_csv(BASE_DIR / 'final_metric_ranking.csv', index=False)\n",
    "print(f'\\nSaved → {BASE_DIR}/final_metric_ranking.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 — Per-indicator clustering: all metrics jointly\n",
    "\n",
    "For each HAT indicator, collect all 10 of its metrics as a mini feature vector\n",
    "(one row per trial, 10 columns = the 10 metrics for that indicator).\n",
    "Then:\n",
    "- Fit **k-means k=2** and a **2-component GMM**\n",
    "- Score against ground-truth labels (ARI, accuracy, silhouette)\n",
    "- This tells you: *as a whole, does this indicator's multi-dimensional signature\n",
    "  separate emotional from neutral?*\n",
    "\n",
    "The per-indicator result is distinct from per-metric: a single metric might not\n",
    "separate cleanly, but the joint 10-D fingerprint of an indicator might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build indicator → list of metric columns mapping\n",
    "def get_indicator_metric_cols(feat_cols):\n",
    "    \"\"\"Return dict: indicator_name → [col1, col2, ...] (all 10 metric cols for it).\"\"\"\n",
    "    ind_map = defaultdict(list)\n",
    "    for col in feat_cols:\n",
    "        indicator, metric = parse_feature(col)\n",
    "        if metric in METRIC_NAMES:\n",
    "            ind_map[indicator].append(col)\n",
    "    return dict(ind_map)\n",
    "\n",
    "ind_map = get_indicator_metric_cols(feat_cols)\n",
    "print(f'Indicators found: {len(ind_map)}')\n",
    "for ind, cols in sorted(ind_map.items()):\n",
    "    print(f'  {ind[:50]:50s}  {len(cols)} metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def cluster_and_score(X_ind: np.ndarray, y_true: np.ndarray, name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fit k=2 KMeans and GMM on X_ind, compare both to ground truth.\n",
    "    Returns a dict of scores.\n",
    "    \"\"\"\n",
    "    result = {'indicator': name, 'n_metrics': X_ind.shape[1]}\n",
    "\n",
    "    # Drop rows with NaN\n",
    "    mask = np.isfinite(X_ind).all(axis=1)\n",
    "    Xc = X_ind[mask]\n",
    "    yc = y_true[mask]\n",
    "    result['n_samples'] = int(mask.sum())\n",
    "\n",
    "    if len(np.unique(yc)) < 2 or Xc.shape[0] < 10:\n",
    "        result.update({'km_ari': np.nan, 'km_acc': np.nan, 'km_sil': np.nan,\n",
    "                       'gmm_ari': np.nan, 'gmm_acc': np.nan, 'gmm_sil': np.nan,\n",
    "                       'km_n_acc': np.nan, 'km_e_acc': np.nan,\n",
    "                       'gmm_n_acc': np.nan, 'gmm_e_acc': np.nan})\n",
    "        return result\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    Xs = sc.fit_transform(Xc)\n",
    "\n",
    "    for tag, model in [('km',  KMeans(n_clusters=2, random_state=42, n_init=20)),\n",
    "                       ('gmm', GaussianMixture(n_components=2, covariance_type='full',\n",
    "                                               random_state=42, n_init=10))]:\n",
    "        lbl = model.fit_predict(Xs)\n",
    "\n",
    "        ari = adjusted_rand_score(yc, lbl)\n",
    "        sil = silhouette_score(Xs, lbl) if len(np.unique(lbl)) > 1 else np.nan\n",
    "\n",
    "        # Align cluster label orientation to ground truth\n",
    "        acc_direct  = (lbl == yc).mean()\n",
    "        acc_flipped = (1 - lbl == yc).mean()\n",
    "        if acc_flipped > acc_direct:\n",
    "            lbl = 1 - lbl\n",
    "        acc = (lbl == yc).mean()\n",
    "\n",
    "        # Per-class accuracy\n",
    "        n_acc = (lbl[yc == 0] == 0).mean() if (yc == 0).sum() > 0 else np.nan\n",
    "        e_acc = (lbl[yc == 1] == 1).mean() if (yc == 1).sum() > 0 else np.nan\n",
    "\n",
    "        result.update({\n",
    "            f'{tag}_ari':   ari,\n",
    "            f'{tag}_acc':   acc,\n",
    "            f'{tag}_sil':   sil,\n",
    "            f'{tag}_n_acc': n_acc,\n",
    "            f'{tag}_e_acc': e_acc,\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ── Run per-indicator multi-metric clustering ──\n",
    "ind_results = []\n",
    "for indicator, cols in ind_map.items():\n",
    "    X_ind = X_raw[cols].values.astype(float)\n",
    "    res = cluster_and_score(X_ind, y, indicator)\n",
    "    ind_results.append(res)\n",
    "\n",
    "ind_df = pd.DataFrame(ind_results).sort_values('gmm_ari', ascending=False).reset_index(drop=True)\n",
    "print(f'Per-indicator clustering complete: {len(ind_df)} indicators')\n",
    "print()\n",
    "print('=== Per-indicator results (sorted by GMM ARI) ===')\n",
    "show_cols = ['indicator','n_metrics','n_samples',\n",
    "             'km_ari','km_acc','km_n_acc','km_e_acc','km_sil',\n",
    "             'gmm_ari','gmm_acc','gmm_n_acc','gmm_e_acc','gmm_sil']\n",
    "print(ind_df[show_cols].to_string(index=False, float_format='{:.3f}'.format))\n",
    "ind_df.to_csv(BASE_DIR / 'per_indicator_clustering.csv', index=False)\n",
    "print(f'\\nSaved → {BASE_DIR}/per_indicator_clustering.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise: horizontal bar chart of GMM accuracy per indicator\n",
    "plot_df = ind_df.dropna(subset=['gmm_acc']).copy()\n",
    "plot_df['short_name'] = plot_df.indicator.apply(lambda s: s.replace('hat_','').replace('irq:','').replace('power/','').replace('msr/','')[:40])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, max(6, len(plot_df) * 0.35)))\n",
    "\n",
    "for ax, col, title, threshold in [\n",
    "    (axes[0], 'gmm_ari',  'GMM ARI (Adjusted Rand Index)', 0),\n",
    "    (axes[1], 'gmm_acc',  'GMM Accuracy vs ground truth',  0.5),\n",
    "]:\n",
    "    vals = plot_df[col].values\n",
    "    colors = ['firebrick' if v > threshold else 'steelblue' for v in vals]\n",
    "    ax.barh(range(len(plot_df)), vals, color=colors, alpha=0.8)\n",
    "    ax.set_yticks(range(len(plot_df)))\n",
    "    ax.set_yticklabels(plot_df.short_name, fontsize=7)\n",
    "    ax.invert_yaxis()\n",
    "    ax.axvline(threshold, color='black', lw=0.8, ls='--')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.suptitle('Per-indicator multi-metric GMM clustering vs ground truth', fontsize=11, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_per_indicator_gmm.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17 — Per-indicator, per-metric clustering\n",
    "\n",
    "Now the finest granularity: for every (indicator, metric) pair, cluster on that\n",
    "single scalar alone (k=2 KMeans) and score against ground truth.\n",
    "\n",
    "This gives us a full **indicator × metric** result table — the definitive answer\n",
    "to *which specific measurement on which specific indicator* drives the separation.\n",
    "\n",
    "We then visualise this as a heatmap (indicators × metrics, coloured by ARI or accuracy),\n",
    "making it immediately clear where the signal lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_cell_results = []\n",
    "\n",
    "for indicator, cols in ind_map.items():\n",
    "    for col in cols:\n",
    "        _, metric = parse_feature(col)\n",
    "        x1d = X_raw[[col]].values.astype(float)\n",
    "        res = cluster_and_score(x1d, y, indicator)\n",
    "        res['metric'] = metric\n",
    "        res['feature'] = col\n",
    "        per_cell_results.append(res)\n",
    "\n",
    "cell_df = pd.DataFrame(per_cell_results)\n",
    "print(f'Per-cell results: {len(cell_df)} (indicator × metric combinations)')\n",
    "\n",
    "# Also retrieve MWU p and effect size from earlier\n",
    "cell_df = cell_df.merge(\n",
    "    mwu_df[['feature','p','effect_r','direction']],\n",
    "    on='feature', how='left'\n",
    ")\n",
    "\n",
    "cell_df_sorted = cell_df.sort_values('gmm_ari', ascending=False).reset_index(drop=True)\n",
    "print('\\n=== Top 40 (indicator, metric) cells by GMM ARI ===')\n",
    "show = ['indicator','metric','gmm_ari','gmm_acc','gmm_n_acc','gmm_e_acc','p','effect_r','direction']\n",
    "print(cell_df_sorted.head(40)[show].to_string(index=False, float_format='{:.4f}'.format))\n",
    "cell_df_sorted.to_csv(BASE_DIR / 'per_indicator_per_metric.csv', index=False)\n",
    "print(f'\\nSaved → {BASE_DIR}/per_indicator_per_metric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Heatmap: indicators × metrics, coloured by GMM ARI ──\n",
    "# Show top 25 indicators by their best single-metric ARI\n",
    "\n",
    "best_ari_per_ind = (\n",
    "    cell_df.groupby('indicator')['gmm_ari']\n",
    "    .max().sort_values(ascending=False)\n",
    "    .head(25).index.tolist()\n",
    ")\n",
    "\n",
    "pivot_ari = cell_df[cell_df.indicator.isin(best_ari_per_ind)].pivot_table(\n",
    "    index='indicator', columns='metric', values='gmm_ari', aggfunc='first'\n",
    ").reindex(index=best_ari_per_ind)\n",
    "\n",
    "pivot_acc = cell_df[cell_df.indicator.isin(best_ari_per_ind)].pivot_table(\n",
    "    index='indicator', columns='metric', values='gmm_acc', aggfunc='first'\n",
    ").reindex(index=best_ari_per_ind)\n",
    "\n",
    "# Clean up indicator names for display\n",
    "def short_ind(s):\n",
    "    return (s.replace('hat_','[/proc] ')\n",
    "             .replace('irq:','').replace('power/','pwr/')\n",
    "             .replace('msr/','msr/').replace('stalled-cycles-','stall-'))[:45]\n",
    "\n",
    "row_labels = [short_ind(s) for s in best_ari_per_ind]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, max(8, len(best_ari_per_ind) * 0.42)))\n",
    "\n",
    "for ax, pivot, title, vmin, vmax, cmap in [\n",
    "    (axes[0], pivot_ari, 'GMM ARI',      -0.3, 0.5, 'RdYlGn'),\n",
    "    (axes[1], pivot_acc, 'GMM Accuracy',  0.4, 1.0, 'RdYlGn'),\n",
    "]:\n",
    "    im = ax.imshow(pivot.values.astype(float), cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.6)\n",
    "    ax.set_xticks(range(len(pivot.columns)))\n",
    "    ax.set_xticklabels(pivot.columns, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_yticks(range(len(pivot.index)))\n",
    "    ax.set_yticklabels(row_labels, fontsize=7)\n",
    "    ax.set_title(title, fontsize=10)\n",
    "\n",
    "    # Annotate cells with value\n",
    "    for i in range(len(pivot.index)):\n",
    "        for j in range(len(pivot.columns)):\n",
    "            val = pivot.values[i, j]\n",
    "            if np.isfinite(val):\n",
    "                ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                        fontsize=5, color='black')\n",
    "\n",
    "plt.suptitle('Per-indicator × per-metric clustering vs ground truth\\n(top 25 indicators by best ARI)',\n",
    "             fontsize=11, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_indicator_metric_cluster_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18 — Which metrics differ most between conditions?\n",
    "\n",
    "Aggregate across all indicators: for each of the 10 metric types, how often\n",
    "does it produce a high-ARI clustering? And what is the mean effect size?\n",
    "\n",
    "This answers: *regardless of which indicator you look at, which measurement\n",
    "approach (burst_clustering, mean_rate, spectral_entropy, …) most reliably\n",
    "discriminates emotional from neutral computation?*\n",
    "\n",
    "We show:\n",
    "1. Mean and max GMM ARI per metric type (averaged over all indicators)\n",
    "2. Mean absolute effect size |r| per metric type\n",
    "3. Hit rate: fraction of indicators where that metric achieves ARI > 0.1\n",
    "4. Distribution plots of the top-2 metric types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_summary = []\n",
    "for m in METRIC_NAMES:\n",
    "    rows = cell_df[cell_df.metric == m].dropna(subset=['gmm_ari'])\n",
    "    if len(rows) == 0:\n",
    "        continue\n",
    "    metric_summary.append({\n",
    "        'metric':          m,\n",
    "        'n_indicators':    len(rows),\n",
    "        'mean_gmm_ari':    rows.gmm_ari.mean(),\n",
    "        'max_gmm_ari':     rows.gmm_ari.max(),\n",
    "        'mean_gmm_acc':    rows.gmm_acc.mean(),\n",
    "        'hit_rate_01':     (rows.gmm_ari > 0.10).mean(),  # fraction with ARI > 0.10\n",
    "        'hit_rate_05':     (rows.gmm_ari > 0.05).mean(),\n",
    "        'mean_abs_effect': rows.effect_r.abs().mean() if 'effect_r' in rows else np.nan,\n",
    "        'best_indicator':  rows.sort_values('gmm_ari', ascending=False).iloc[0]['indicator'],\n",
    "    })\n",
    "\n",
    "msummary_df = pd.DataFrame(metric_summary).sort_values('mean_gmm_ari', ascending=False).reset_index(drop=True)\n",
    "print('=== Metric type discriminability (across all indicators) ===')\n",
    "print(msummary_df.drop(columns=['best_indicator']).to_string(index=False, float_format='{:.4f}'.format))\n",
    "print()\n",
    "print('Best indicator per metric:')\n",
    "for _, row in msummary_df.iterrows():\n",
    "    print(f'  {row.metric:20s}  best: {row.best_indicator[:60]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: mean GMM ARI and hit rate per metric type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "order = msummary_df['metric'].tolist()\n",
    "x = np.arange(len(order))\n",
    "\n",
    "axes[0].bar(x, msummary_df['mean_gmm_ari'], color='steelblue', alpha=0.85)\n",
    "axes[0].bar(x, msummary_df['max_gmm_ari'],  color='firebrick', alpha=0.35, label='max ARI')\n",
    "axes[0].set_xticks(x); axes[0].set_xticklabels(order, rotation=40, ha='right', fontsize=8)\n",
    "axes[0].set_ylabel('GMM ARI'); axes[0].set_title('Mean / Max GMM ARI per metric type')\n",
    "axes[0].axhline(0, color='gray', lw=0.5, ls='--')\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "axes[1].bar(x, msummary_df['hit_rate_01'], color='forestgreen', alpha=0.85, label='ARI>0.10')\n",
    "axes[1].bar(x, msummary_df['hit_rate_05'], color='lightgreen',  alpha=0.6,  label='ARI>0.05')\n",
    "axes[1].set_xticks(x); axes[1].set_xticklabels(order, rotation=40, ha='right', fontsize=8)\n",
    "axes[1].set_ylabel('Fraction of indicators'); axes[1].set_title('Hit rate: ARI > threshold')\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "axes[2].bar(x, msummary_df['mean_abs_effect'].fillna(0), color='darkorange', alpha=0.85)\n",
    "axes[2].set_xticks(x); axes[2].set_xticklabels(order, rotation=40, ha='right', fontsize=8)\n",
    "axes[2].set_ylabel('Mean |rank-biserial r|'); axes[2].set_title('Mean effect size per metric type')\n",
    "\n",
    "plt.suptitle('Which metric type best discriminates Emotional vs Neutral?', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_metric_discriminability.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Distribution comparison for top-2 metric types (across all indicators, pooled) ──\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "top2_metrics = msummary_df.head(2)['metric'].tolist()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, m in zip(axes, top2_metrics):\n",
    "    m_cols = [c for c in feat_cols if c.endswith(f'__{m}')]\n",
    "    # Pool all indicators for this metric type\n",
    "    n_pool = X_raw.loc[y == 0, m_cols].values.ravel()\n",
    "    e_pool = X_raw.loc[y == 1, m_cols].values.ravel()\n",
    "    n_pool = n_pool[np.isfinite(n_pool)]\n",
    "    e_pool = e_pool[np.isfinite(e_pool)]\n",
    "\n",
    "    # Clip extreme outliers for display (1st–99th percentile)\n",
    "    lo = min(np.percentile(n_pool, 1), np.percentile(e_pool, 1))\n",
    "    hi = max(np.percentile(n_pool, 99), np.percentile(e_pool, 99))\n",
    "    xr = np.linspace(lo, hi, 300)\n",
    "\n",
    "    for vals, label, color in [\n",
    "        (n_pool, 'Neutral',   'steelblue'),\n",
    "        (e_pool, 'Emotional', 'firebrick'),\n",
    "    ]:\n",
    "        kde = gaussian_kde(np.clip(vals, lo, hi), bw_method='scott')\n",
    "        ax.fill_between(xr, kde(xr), alpha=0.35, color=color, label=label)\n",
    "        ax.plot(xr, kde(xr), color=color, lw=1.5)\n",
    "\n",
    "    # Report Mann-Whitney over pooled values\n",
    "    _, p_pool = sp_stats.mannwhitneyu(n_pool, e_pool, alternative='two-sided')\n",
    "    ax.set_title(f'Metric: {m} (pooled across all indicators)\\np={p_pool:.3e}', fontsize=9)\n",
    "    ax.set_xlabel('Metric value')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Distribution of top-2 metric types: Neutral vs Emotional (all indicators pooled)',\n",
    "             fontsize=10, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_top_metric_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19 — Confusion matrices for best-performing (indicator, metric) pairs\n",
    "\n",
    "For the top N cells by GMM ARI, show the full confusion matrix:\n",
    "how many neutral trials were assigned to each cluster,\n",
    "how many emotional trials were assigned to each cluster.\n",
    "\n",
    "This shows whether an indicator separates *both* conditions well,\n",
    "or only one (e.g. perfectly identifies emotional but misclassifies neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_CM = 6  # how many confusion matrices to plot\n",
    "top_cells = cell_df_sorted.head(TOP_CM)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (_, row) in enumerate(top_cells.iterrows()):\n",
    "    ax = axes[i]\n",
    "    col = row['feature']\n",
    "    x1d = X_raw[[col]].values.astype(float)\n",
    "    mask = np.isfinite(x1d).ravel()\n",
    "    Xc = x1d[mask]; yc = y[mask]\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    Xs = sc.fit_transform(Xc)\n",
    "    gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42, n_init=10)\n",
    "    lbl = gmm.fit_predict(Xs)\n",
    "\n",
    "    # Align orientation\n",
    "    if (1 - lbl == yc).mean() > (lbl == yc).mean():\n",
    "        lbl = 1 - lbl\n",
    "\n",
    "    cm = confusion_matrix(yc, lbl)\n",
    "    # rows: true label (0=neutral, 1=emotional), cols: predicted cluster\n",
    "    im = ax.imshow(cm, cmap='Blues', vmin=0)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_xticks([0, 1]); ax.set_xticklabels(['Cluster 0', 'Cluster 1'])\n",
    "    ax.set_yticks([0, 1]); ax.set_yticklabels(['Neutral (GT)', 'Emotional (GT)'])\n",
    "    for r in range(2):\n",
    "        for c in range(2):\n",
    "            ax.text(c, r, str(cm[r, c]), ha='center', va='center',\n",
    "                    fontsize=13, fontweight='bold',\n",
    "                    color='white' if cm[r, c] > cm.max() * 0.5 else 'black')\n",
    "    ind_short = short_ind(row['indicator'])\n",
    "    ax.set_title(\n",
    "        f'{ind_short}\\nmetric: {row[\"metric\"]}\\n'\n",
    "        f'ARI={row[\"gmm_ari\"]:.3f}  acc={row[\"gmm_acc\"]:.1%}',\n",
    "        fontsize=7\n",
    "    )\n",
    "\n",
    "plt.suptitle(f'Confusion matrices — top {TOP_CM} (indicator, metric) pairs by GMM ARI',\n",
    "             fontsize=11, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'fig_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 — Final results table\n",
    "\n",
    "One unified table: every (indicator, metric) pair, with:\n",
    "- GMM ARI and accuracy (overall + per condition)\n",
    "- MWU p-value and effect size\n",
    "- Length-sensitivity flag\n",
    "- Sorted by GMM ARI descending\n",
    "\n",
    "This is the primary output for thesis reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = cell_df_sorted[[\n",
    "    'indicator','metric','feature',\n",
    "    'gmm_ari','gmm_acc','gmm_n_acc','gmm_e_acc','gmm_sil',\n",
    "    'km_ari','km_acc',\n",
    "    'p','effect_r','direction','n_samples'\n",
    "]].copy()\n",
    "\n",
    "non_normalised = ['variance', 'p90_p10', 'lz_complexity', 'perm_entropy']\n",
    "final['length_sensitive'] = final.metric.isin(non_normalised) & LENGTH_CONFOUNDED\n",
    "\n",
    "print('=== Full (indicator × metric) results — top 50 ===')\n",
    "print(final.head(50).to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "final.to_csv(BASE_DIR / 'full_indicator_metric_results.csv', index=False)\n",
    "print(f'\\nSaved → {BASE_DIR}/full_indicator_metric_results.csv')\n",
    "\n",
    "# Quick text summary\n",
    "print('\\n=== KEY FINDINGS ===')\n",
    "top5 = final.head(5)\n",
    "for _, r in top5.iterrows():\n",
    "    flag = ' [LENGTH-SENSITIVE]' if r.length_sensitive else ''\n",
    "    print(f'  {r.indicator[:35]:35s}  {r.metric:20s}  '\n",
    "          f'ARI={r.gmm_ari:.3f}  acc={r.gmm_acc:.1%}  '\n",
    "          f'p={r.p:.3e}  {r.direction}{flag}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
