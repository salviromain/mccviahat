{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02468b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PROMPTS_DIR = Path.home() / 'Desktop' / 'mccviahat' / 'prompts'\n",
    "DATA_DIR    = Path.home() / 'Desktop' / 'mccviahat' / 'data'\n",
    "PROMPTS_DIR.mkdir(exist_ok=True)\n",
    "print(f'Output dir: {PROMPTS_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8e3e7",
   "metadata": {},
   "source": [
    "## 1 — Neutral prompts from WikiHow\n",
    "\n",
    "Load the HuggingFace dataset, keep only English rows where the response\n",
    "is under 500 tokens (estimated as `len(text.split())` × 1.3), then\n",
    "randomly sample 5 and format as the expected JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bebdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiHow dataset from HuggingFace\n",
    "df_wiki = pd.read_parquet(\n",
    "    \"hf://datasets/0x22almostEvil/multilingual-wikihow-qa-16k/data/train-00000-of-00001-0bdf6bc5b4b507e0.parquet\"\n",
    ")\n",
    "print(f'Raw dataset: {len(df_wiki):,} rows')\n",
    "print(f'Columns: {list(df_wiki.columns)}')\n",
    "df_wiki.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f894275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect columns to find the right text fields\n",
    "for col in df_wiki.columns:\n",
    "    sample = df_wiki[col].dropna().iloc[0] if df_wiki[col].notna().any() else '(all NaN)'\n",
    "    preview = str(sample)[:120]\n",
    "    print(f'{col:30s}  {preview}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate token count (rough: words × 1.3 for subword tokenisation)\n",
    "# Adjust the column name below after inspecting the dataset\n",
    "# Common names: 'response', 'answer', 'text', 'RESPONSE'\n",
    "\n",
    "# Auto-detect the response column\n",
    "response_col = None\n",
    "for candidate in ['response', 'RESPONSE', 'answer', 'ANSWER', 'text']:\n",
    "    if candidate in df_wiki.columns:\n",
    "        response_col = candidate\n",
    "        break\n",
    "if response_col is None:\n",
    "    # Fall back to the last string column\n",
    "    str_cols = [c for c in df_wiki.columns if df_wiki[c].dtype == 'object']\n",
    "    response_col = str_cols[-1] if str_cols else df_wiki.columns[-1]\n",
    "print(f'Using response column: \"{response_col}\"')\n",
    "\n",
    "# Auto-detect the title/question column\n",
    "title_col = None\n",
    "for candidate in ['question', 'QUESTION', 'title', 'TITLE', 'topic']:\n",
    "    if candidate in df_wiki.columns:\n",
    "        title_col = candidate\n",
    "        break\n",
    "if title_col is None:\n",
    "    str_cols = [c for c in df_wiki.columns if df_wiki[c].dtype == 'object']\n",
    "    title_col = str_cols[0] if str_cols else df_wiki.columns[0]\n",
    "print(f'Using title column:    \"{title_col}\"')\n",
    "\n",
    "# Filter: English-only if there's a language column\n",
    "lang_col = None\n",
    "for candidate in ['language', 'lang', 'LANGUAGE']:\n",
    "    if candidate in df_wiki.columns:\n",
    "        lang_col = candidate\n",
    "        break\n",
    "\n",
    "if lang_col:\n",
    "    df_wiki = df_wiki[df_wiki[lang_col].str.lower().str.startswith('en')].copy()\n",
    "    print(f'After English filter: {len(df_wiki):,} rows')\n",
    "\n",
    "# Estimate tokens and filter\n",
    "df_wiki['_est_tokens'] = df_wiki[response_col].fillna('').apply(lambda x: int(len(str(x).split()) * 1.3))\n",
    "df_wiki_short = df_wiki[df_wiki['_est_tokens'] < 500].copy()\n",
    "print(f'After <500 token filter: {len(df_wiki_short):,} rows')\n",
    "print(f'Token range: {df_wiki_short[\"_est_tokens\"].min()}–{df_wiki_short[\"_est_tokens\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 prompts (fix seed for reproducibility)\n",
    "SEED = 42\n",
    "wiki_sample = df_wiki_short.sample(n=5, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "neutral_prompts = []\n",
    "for i, row in wiki_sample.iterrows():\n",
    "    neutral_prompts.append({\n",
    "        'id': i + 1,\n",
    "        'title': str(row[title_col]).strip()[:80],\n",
    "        'instructions': str(row[response_col]).strip(),\n",
    "    })\n",
    "\n",
    "# Preview\n",
    "for p in neutral_prompts:\n",
    "    est_tok = int(len(p['instructions'].split()) * 1.3)\n",
    "    print(f\"  [{p['id']}] {p['title'][:60]:60s}  ~{est_tok} tokens  ({len(p['instructions'])} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e43114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neutral prompts\n",
    "neutral_path = PROMPTS_DIR / 'neutral_wikihow.json'\n",
    "neutral_path.write_text(json.dumps(neutral_prompts, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "print(f'Saved {len(neutral_prompts)} neutral prompts → {neutral_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb51bbe",
   "metadata": {},
   "source": [
    "## 2 — Emotional prompts from Creepypasta\n",
    "\n",
    "Load the Excel file, filter to stories where `estimated_reading_time < 5` minutes,\n",
    "then sample 5 and format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f658028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load creepypasta dataset\n",
    "xlsx_path = DATA_DIR / 'creepypastas.xlsx'\n",
    "assert xlsx_path.exists(), f'Missing: {xlsx_path}'\n",
    "\n",
    "df_creepy = pd.read_excel(xlsx_path)\n",
    "print(f'Raw creepypasta rows: {len(df_creepy):,}')\n",
    "print(f'Columns: {list(df_creepy.columns)}')\n",
    "df_creepy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect columns\n",
    "for col in df_creepy.columns:\n",
    "    sample = df_creepy[col].dropna().iloc[0] if df_creepy[col].notna().any() else '(all NaN)'\n",
    "    preview = str(sample)[:120]\n",
    "    print(f'{col:35s}  {preview}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36815d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the reading time column (case-insensitive search)\n",
    "time_col = None\n",
    "for c in df_creepy.columns:\n",
    "    if 'reading_time' in c.lower() or 'readingtime' in c.lower() or 'reading time' in c.lower():\n",
    "        time_col = c\n",
    "        break\n",
    "if time_col is None:\n",
    "    # Try partial match\n",
    "    for c in df_creepy.columns:\n",
    "        if 'time' in c.lower() and 'read' in c.lower():\n",
    "            time_col = c\n",
    "            break\n",
    "print(f'Reading time column: \"{time_col}\"')\n",
    "print(f'Sample values: {df_creepy[time_col].head(10).tolist()}')\n",
    "\n",
    "# The column might be a string like \"3 min\" or a number — handle both\n",
    "if df_creepy[time_col].dtype == 'object':\n",
    "    # Extract numeric part\n",
    "    df_creepy['_reading_min'] = df_creepy[time_col].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "else:\n",
    "    df_creepy['_reading_min'] = df_creepy[time_col].astype(float)\n",
    "\n",
    "print(f'Reading time range: {df_creepy[\"_reading_min\"].min():.1f}–{df_creepy[\"_reading_min\"].max():.1f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d49b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: estimated_reading_time < 5 minutes\n",
    "df_creepy_short = df_creepy[df_creepy['_reading_min'] < 5].copy()\n",
    "print(f'After <5 min filter: {len(df_creepy_short):,} rows')\n",
    "\n",
    "# Auto-detect title and body columns\n",
    "creepy_title_col = None\n",
    "for candidate in ['title', 'Title', 'TITLE', 'name', 'Name']:\n",
    "    if candidate in df_creepy.columns:\n",
    "        creepy_title_col = candidate\n",
    "        break\n",
    "if creepy_title_col is None:\n",
    "    str_cols = [c for c in df_creepy.columns if df_creepy[c].dtype == 'object']\n",
    "    creepy_title_col = str_cols[0]\n",
    "\n",
    "creepy_body_col = None\n",
    "for candidate in ['body', 'Body', 'text', 'Text', 'story', 'Story', 'content', 'Content']:\n",
    "    if candidate in df_creepy.columns:\n",
    "        creepy_body_col = candidate\n",
    "        break\n",
    "if creepy_body_col is None:\n",
    "    # Pick the string column with the longest average length\n",
    "    str_cols = [c for c in df_creepy.columns if df_creepy[c].dtype == 'object']\n",
    "    creepy_body_col = max(str_cols, key=lambda c: df_creepy[c].str.len().mean())\n",
    "\n",
    "print(f'Title column: \"{creepy_title_col}\"')\n",
    "print(f'Body column:  \"{creepy_body_col}\"')\n",
    "\n",
    "# Estimate tokens for the body text\n",
    "df_creepy_short['_est_tokens'] = df_creepy_short[creepy_body_col].fillna('').apply(\n",
    "    lambda x: int(len(str(x).split()) * 1.3)\n",
    ")\n",
    "print(f'Token range: {df_creepy_short[\"_est_tokens\"].min()}–{df_creepy_short[\"_est_tokens\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 creepypasta prompts\n",
    "creepy_sample = df_creepy_short.sample(n=5, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "emotional_prompts = []\n",
    "for i, row in creepy_sample.iterrows():\n",
    "    emotional_prompts.append({\n",
    "        'id': i + 1,\n",
    "        'title': str(row[creepy_title_col]).strip()[:80],\n",
    "        'instructions': str(row[creepy_body_col]).strip(),\n",
    "    })\n",
    "\n",
    "# Preview\n",
    "for p in emotional_prompts:\n",
    "    est_tok = int(len(p['instructions'].split()) * 1.3)\n",
    "    print(f\"  [{p['id']}] {p['title'][:60]:60s}  ~{est_tok} tokens  ({len(p['instructions'])} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e020cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save emotional prompts\n",
    "emotional_path = PROMPTS_DIR / 'emotional_creepypasta.json'\n",
    "emotional_path.write_text(json.dumps(emotional_prompts, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "print(f'Saved {len(emotional_prompts)} emotional prompts → {emotional_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97857012",
   "metadata": {},
   "source": [
    "## 3 — Summary & Token-Count Comparison\n",
    "\n",
    "Side-by-side comparison to check that the two sets are roughly matched\n",
    "in length — important for reducing confounders in the HAT experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_toks = [int(len(p['instructions'].split()) * 1.3) for p in neutral_prompts]\n",
    "e_toks = [int(len(p['instructions'].split()) * 1.3) for p in emotional_prompts]\n",
    "\n",
    "print(f'{\"\":20s}  {\"Neutral (WikiHow)\":>20s}  {\"Emotional (Creepypasta)\":>24s}')\n",
    "print('-' * 70)\n",
    "print(f'{\"Mean tokens\":20s}  {np.mean(n_toks):20.0f}  {np.mean(e_toks):24.0f}')\n",
    "print(f'{\"Std tokens\":20s}  {np.std(n_toks):20.0f}  {np.std(e_toks):24.0f}')\n",
    "print(f'{\"Min tokens\":20s}  {np.min(n_toks):20d}  {np.min(e_toks):24d}')\n",
    "print(f'{\"Max tokens\":20s}  {np.max(n_toks):20d}  {np.max(e_toks):24d}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "x = np.arange(5)\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, n_toks, w, label='Neutral (WikiHow)', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + w/2, e_toks, w, label='Emotional (Creepypasta)', color='firebrick', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Prompt {i+1}' for i in range(5)])\n",
    "ax.set_ylabel('Estimated tokens')\n",
    "ax.set_title('Token Count Comparison: Neutral vs Emotional')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ratio = np.mean(e_toks) / np.mean(n_toks)\n",
    "print(f'\\nMean ratio (emotional/neutral): {ratio:.2f}')\n",
    "if 0.8 <= ratio <= 1.2:\n",
    "    print('✓ Conditions are well-matched in token count')\n",
    "else:\n",
    "    print(f'⚠ Token counts differ by {abs(ratio-1)*100:.0f}% — consider re-sampling or trimming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29af99c",
   "metadata": {},
   "source": [
    "## Output files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `prompts/neutral_wikihow.json` | 5 WikiHow how-to instructions (<500 tokens each) |\n",
    "| `prompts/emotional_creepypasta.json` | 5 creepypasta stories (<5 min reading time) |\n",
    "\n",
    "Usage:\n",
    "```bash\n",
    "python3 scripts/run_prompts_json.py --json prompts/neutral_wikihow.json --label neutral\n",
    "python3 scripts/run_prompts_json.py --json prompts/emotional_creepypasta.json --label emotional\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
