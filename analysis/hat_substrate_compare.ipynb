{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2112ff6c",
   "metadata": {},
   "source": [
    "# HAT Substrate Comparison: Neutral vs Emotional\n",
    "\n",
    "This notebook compares **system-wide substrate signals** collected per run:\n",
    "- `perf_stat.txt` (1 ms buckets from `sudo perf stat -I 1 ...`)\n",
    "- `kernel_log.txt` (kernel log slice for the run window)\n",
    "- `responses.jsonl` (request timing + success)\n",
    "\n",
    "It intentionally **does not** treat `/proc` metrics as evidence, except for optional alignment/sanity.\n",
    "\n",
    "You will set:\n",
    "- `BASE_DIR`\n",
    "- `NEUTRAL_RUN`\n",
    "- `EMOTIONAL_RUN`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ca55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd5db3",
   "metadata": {},
   "source": [
    "## 1 — Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.home() / \"Desktop\" / \"mccviahat\" / \"mccviahat_runs\"\n",
    "\n",
    "NEUTRAL_RUN   = \"2026-02-09T22-28-54_neutral\"\n",
    "EMOTIONAL_RUN = \"2026-02-09T22-18-50_emotional\"\n",
    "\n",
    "n_dir = BASE_DIR / NEUTRAL_RUN\n",
    "e_dir = BASE_DIR / EMOTIONAL_RUN\n",
    "\n",
    "for d in [n_dir, e_dir]:\n",
    "    assert d.exists(), f\"Missing: {d}\"\n",
    "\n",
    "print(\"Neutral:\", n_dir.name)\n",
    "print(\"Emotional:\", e_dir.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78681c3",
   "metadata": {},
   "source": [
    "## 2 — Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Meta & responses ──\n",
    "\n",
    "def load_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def load_responses(run_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for line in (run_dir / \"responses.jsonl\").read_text(encoding=\"utf-8\").splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        r = json.loads(line)\n",
    "        # extract llama.cpp timings from the nested response JSON\n",
    "        try:\n",
    "            resp = json.loads(r[\"response_raw\"])\n",
    "            timings = resp.get(\"timings\", {})\n",
    "            r[\"prompt_ms\"] = timings.get(\"prompt_ms\")\n",
    "            r[\"predicted_ms\"] = timings.get(\"predicted_ms\")\n",
    "            r[\"tokens_evaluated\"] = resp.get(\"tokens_evaluated\")\n",
    "            r[\"tokens_predicted\"] = resp.get(\"tokens_predicted\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        rows.append(r)\n",
    "    df = pd.DataFrame(rows)\n",
    "    keep = [c for c in [\"id\",\"title\",\"ok\",\"t_request_start_ns\",\"t_request_end_ns\",\n",
    "                         \"prompt_ms\",\"predicted_ms\",\"tokens_evaluated\",\"tokens_predicted\"] if c in df.columns]\n",
    "    df = df[keep].sort_values(\"t_request_start_ns\").reset_index(drop=True)\n",
    "    df[\"duration_s\"] = (df[\"t_request_end_ns\"] - df[\"t_request_start_ns\"]) / 1e9\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192bdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── perf stat parser ──\n",
    "#\n",
    "# Format per line (16 events x ~6000 buckets = ~97k lines):\n",
    "#   0.001082375      1        irq:irq_handler_entry    #   1.536 /sec\n",
    "#   <timestamp_s>  <count>    <event_name>             # <rate> <unit>\n",
    "#\n",
    "# Some counts contain commas (e.g. 88,790).  Some are floats (e.g. 651.08 msec cpu-clock).\n",
    "# We extract: (timestamp_s, count_or_value, event_name).\n",
    "\n",
    "def parse_perf_stat(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse `perf stat -I` output into a tidy DataFrame with columns: t_s, event, value.\"\"\"\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    data = []\n",
    "    # Match: <timestamp_s>  <value> [unit]  <event_name>\n",
    "    # The '#' comment at the end is optional.\n",
    "    pat = re.compile(\n",
    "        r\"^\\s*\"\n",
    "        r\"(\\d+\\.\\d+)\"          # group 1: timestamp in seconds\n",
    "        r\"\\s+\"\n",
    "        r\"([\\d,\\.]+|<[^>]+>)\"  # group 2: count (possibly with commas) or <not counted>\n",
    "        r\"\\s+\"\n",
    "        r\"(?:\\S+\\s+)?\"         # optional unit (msec, Joules, C, etc.)\n",
    "        r\"(\\S+)\"              # group 3: event name\n",
    "        r\"\\s*\",\n",
    "        re.MULTILINE\n",
    "    )\n",
    "    for m in pat.finditer(text):\n",
    "        t_s = float(m.group(1))\n",
    "        raw = m.group(2).replace(\",\", \"\")\n",
    "        evt = m.group(3)\n",
    "        if raw.startswith(\"<\"):\n",
    "            val = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                val = float(raw)\n",
    "            except ValueError:\n",
    "                val = np.nan\n",
    "        data.append((t_s, evt, val))\n",
    "    return pd.DataFrame(data, columns=[\"t_s\", \"event\", \"value\"])\n",
    "\n",
    "\n",
    "def perf_to_wide(tidy: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Pivot tidy perf data to wide format: one row per timestamp, one column per event.\"\"\"\n",
    "    w = tidy.pivot_table(index=\"t_s\", columns=\"event\", values=\"value\", aggfunc=\"first\")\n",
    "    w = w.sort_index().reset_index()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ed0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── proc_system_sample.csv loader ──\n",
    "#\n",
    "# This CSV has a very wide header with /proc/interrupts IRQ numbers, softirq types,\n",
    "# PSI fields, net/disk counters, and CPU freq.\n",
    "# All values are cumulative counters (except PSI avgs and freq).\n",
    "\n",
    "def load_proc_system(run_dir: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(run_dir / \"proc_system_sample.csv\")\n",
    "    # Convert timestamp to relative seconds from first sample\n",
    "    df[\"t_s\"] = (df[\"timestamp_ns\"] - df[\"timestamp_ns\"].iloc[0]) / 1e9\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_proc_sample(run_dir: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(run_dir / \"proc_sample.csv\")\n",
    "    df[\"t_s\"] = (df[\"timestamp_ns\"] - df[\"timestamp_ns\"].iloc[0]) / 1e9\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add74e60",
   "metadata": {},
   "source": [
    "## 3 — Load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta\n",
    "n_meta  = load_json(n_dir / \"meta.json\")\n",
    "e_meta  = load_json(e_dir / \"meta.json\")\n",
    "n_cmeta = load_json(n_dir / \"collector_meta.json\")\n",
    "e_cmeta = load_json(e_dir / \"collector_meta.json\")\n",
    "\n",
    "# Responses\n",
    "n_resp = load_responses(n_dir)\n",
    "e_resp = load_responses(e_dir)\n",
    "\n",
    "# Perf (1 ms resolution)\n",
    "n_perf_tidy = parse_perf_stat(n_dir / \"perf_stat.txt\")\n",
    "e_perf_tidy = parse_perf_stat(e_dir / \"perf_stat.txt\")\n",
    "n_perf = perf_to_wide(n_perf_tidy)\n",
    "e_perf = perf_to_wide(e_perf_tidy)\n",
    "\n",
    "# /proc system (200 ms resolution)\n",
    "n_sys = load_proc_system(n_dir)\n",
    "e_sys = load_proc_system(e_dir)\n",
    "\n",
    "# /proc process (200 ms resolution)\n",
    "n_proc = load_proc_sample(n_dir)\n",
    "e_proc = load_proc_sample(e_dir)\n",
    "\n",
    "print(f\"Perf events: {sorted(n_perf_tidy['event'].unique())}\")\n",
    "print(f\"Perf buckets — neutral: {len(n_perf):,}  emotional: {len(e_perf):,}\")\n",
    "print(f\"Proc system rows — neutral: {len(n_sys)}  emotional: {len(e_sys)}\")\n",
    "print(f\"Requests — neutral: {len(n_resp)}  emotional: {len(e_resp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81130247",
   "metadata": {},
   "source": [
    "## 4 — Request timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_windows(resp: pd.DataFrame, t0_ns: int) -> list[tuple[float, float]]:\n",
    "    \"\"\"Return list of (start_s, end_s) relative to collector t0.\"\"\"\n",
    "    wins = []\n",
    "    for _, r in resp.iterrows():\n",
    "        s = (r[\"t_request_start_ns\"] - t0_ns) / 1e9\n",
    "        e = (r[\"t_request_end_ns\"]   - t0_ns) / 1e9\n",
    "        wins.append((s, e))\n",
    "    return wins\n",
    "\n",
    "n_t0 = n_cmeta[\"t0_ns\"]\n",
    "e_t0 = e_cmeta[\"t0_ns\"]\n",
    "n_wins = request_windows(n_resp, n_t0)\n",
    "e_wins = request_windows(e_resp, e_t0)\n",
    "\n",
    "print(\"=== Neutral ===\")\n",
    "display(n_resp[[\"id\", \"title\", \"duration_s\", \"tokens_evaluated\", \"tokens_predicted\", \"prompt_ms\", \"predicted_ms\"]])\n",
    "print(\"\\n=== Emotional ===\")\n",
    "display(e_resp[[\"id\", \"title\", \"duration_s\", \"tokens_evaluated\", \"tokens_predicted\", \"prompt_ms\", \"predicted_ms\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ba34d",
   "metadata": {},
   "source": [
    "## 5 — Perf event exploration (1 ms resolution)\n",
    "\n",
    "These are the high-resolution signals from `perf stat -I 1`. Each row is a ~1 ms bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shade_requests(ax, wins, color=\"C0\", alpha=0.08):\n",
    "    for s, e in wins:\n",
    "        ax.axvspan(s, e, color=color, alpha=alpha)\n",
    "\n",
    "def plot_perf_event(event: str, smooth_ms: int = 50):\n",
    "    \"\"\"Plot a single perf event for both conditions, with request windows shaded.\"\"\"\n",
    "    if event not in n_perf.columns or event not in e_perf.columns:\n",
    "        print(f\"Event '{event}' not found in both runs.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "    for ax, perf, wins, label, color in [\n",
    "        (axes[0], n_perf, n_wins, \"neutral\", \"C0\"),\n",
    "        (axes[1], e_perf, e_wins, \"emotional\", \"C1\"),\n",
    "    ]:\n",
    "        y = perf[event]\n",
    "        # Rolling mean: estimate samples per smooth_ms window\n",
    "        dt_ms = np.median(np.diff(perf[\"t_s\"].values)) * 1000 if len(perf) > 2 else 1.0\n",
    "        w = max(1, int(round(smooth_ms / dt_ms)))\n",
    "        y_smooth = y.rolling(w, min_periods=1).mean()\n",
    "\n",
    "        ax.plot(perf[\"t_s\"], y_smooth, linewidth=0.6, color=color, label=label)\n",
    "        shade_requests(ax, wins, color=color)\n",
    "        ax.set_title(f\"{label} — {event}\")\n",
    "        ax.set_xlabel(\"time (s)\")\n",
    "        ax.grid(True, alpha=0.2)\n",
    "\n",
    "    axes[0].set_ylabel(f\"count / {smooth_ms}ms avg\")\n",
    "    fig.suptitle(event, fontweight=\"bold\", y=1.02)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ad591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRQ events\n",
    "irq_events = [\n",
    "    \"irq:irq_handler_entry\",\n",
    "    \"irq:softirq_entry\",\n",
    "    \"irq:softirq_raise\",\n",
    "    \"tlb:tlb_flush\",\n",
    "]\n",
    "for evt in irq_events:\n",
    "    plot_perf_event(evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Software counters\n",
    "sw_events = [\"context-switches\", \"cpu-migrations\", \"page-faults\"]\n",
    "for evt in sw_events:\n",
    "    plot_perf_event(evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power / thermal\n",
    "power_events = [e for e in n_perf.columns if \"power\" in e or \"energy\" in e or \"thermal\" in e or \"throttle\" in e]\n",
    "for evt in power_events:\n",
    "    plot_perf_event(evt, smooth_ms=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28256933",
   "metadata": {},
   "source": [
    "## 6 — Distribution comparison (perf events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f831d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(events: list[str], bins: int = 80):\n",
    "    n_evts = len(events)\n",
    "    fig, axes = plt.subplots(1, n_evts, figsize=(5 * n_evts, 4))\n",
    "    if n_evts == 1:\n",
    "        axes = [axes]\n",
    "    for ax, evt in zip(axes, events):\n",
    "        if evt not in n_perf.columns or evt not in e_perf.columns:\n",
    "            ax.set_title(f\"{evt}\\n(missing)\")\n",
    "            continue\n",
    "        n_vals = n_perf[evt].dropna().values\n",
    "        e_vals = e_perf[evt].dropna().values\n",
    "        ax.hist(n_vals, bins=bins, density=True, alpha=0.5, label=\"neutral\")\n",
    "        ax.hist(e_vals, bins=bins, density=True, alpha=0.5, label=\"emotional\")\n",
    "        ax.set_title(evt)\n",
    "        ax.set_xlabel(\"count per 1ms bucket\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_distributions([\"irq:irq_handler_entry\", \"irq:softirq_entry\", \"tlb:tlb_flush\"])\n",
    "plot_distributions([\"context-switches\", \"cpu-migrations\", \"page-faults\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb35bc5",
   "metadata": {},
   "source": [
    "## 7 — Descriptive statistics (perf events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927aa537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(series: pd.Series) -> dict:\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    return {\n",
    "        \"n\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"std\": s.std(),\n",
    "        \"median\": s.median(),\n",
    "        \"p95\": s.quantile(0.95),\n",
    "        \"p99\": s.quantile(0.99),\n",
    "        \"max\": s.max(),\n",
    "        \"sum\": s.sum(),\n",
    "    }\n",
    "\n",
    "all_events = sorted(set(n_perf.columns) & set(e_perf.columns) - {\"t_s\"})\n",
    "rows = []\n",
    "for evt in all_events:\n",
    "    nd = describe(n_perf[evt])\n",
    "    ed = describe(e_perf[evt])\n",
    "    rows.append({\n",
    "        \"event\": evt,\n",
    "        **{f\"n_{k}\": v for k, v in nd.items()},\n",
    "        **{f\"e_{k}\": v for k, v in ed.items()},\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows).set_index(\"event\")\n",
    "summary.style.format(\"{:.2f}\", na_rep=\"—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82f081",
   "metadata": {},
   "source": [
    "## 8 — /proc/interrupts & /proc/softirqs (200 ms resolution)\n",
    "\n",
    "These are cumulative counters. We take first-differences to get rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify softirq columns (named types like HI, TIMER, NET_RX, etc.)\n",
    "softirq_types = [\"HI\", \"TIMER\", \"NET_TX\", \"NET_RX\", \"BLOCK\", \"IRQ_POLL\",\n",
    "                 \"TASKLET\", \"SCHED\", \"HRTIMER\", \"RCU\"]\n",
    "softirq_cols = [c for c in softirq_types if c in n_sys.columns and c in e_sys.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "for ax, sys_df, wins, label, color in [\n",
    "    (axes[0], n_sys, n_wins, \"neutral\", \"C0\"),\n",
    "    (axes[1], e_sys, e_wins, \"emotional\", \"C1\"),\n",
    "]:\n",
    "    for col in softirq_cols:\n",
    "        rate = sys_df[col].diff() / sys_df[\"t_s\"].diff()  # counts per second\n",
    "        ax.plot(sys_df[\"t_s\"], rate, linewidth=0.8, label=col)\n",
    "    shade_requests(ax, wins, color=color)\n",
    "    ax.set_title(f\"{label} — /proc/softirqs rate (counts/s)\")\n",
    "    ax.set_ylabel(\"rate\")\n",
    "    ax.legend(fontsize=7, ncol=5, loc=\"upper right\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "axes[1].set_xlabel(\"time (s)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0df96",
   "metadata": {},
   "source": [
    "## 9 — PSI Pressure (CPU, Memory, I/O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSI columns have duplicated names (cpu, memory, io all have some_avg10 etc.)\n",
    "# The collector writes them in order: cpu pressure, then memory, then io.\n",
    "# We identify them by position in the header.\n",
    "\n",
    "psi_cols = [c for c in n_sys.columns if c.startswith((\"some_\", \"full_\"))]\n",
    "print(f\"PSI columns found: {psi_cols}\")\n",
    "\n",
    "if psi_cols:\n",
    "    # Just plot some_avg10 for quick overview (first occurrence = cpu)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "    for ax, sys_df, wins, label, color in [\n",
    "        (axes[0], n_sys, n_wins, \"neutral\", \"C0\"),\n",
    "        (axes[1], e_sys, e_wins, \"emotional\", \"C1\"),\n",
    "    ]:\n",
    "        for col in psi_cols:\n",
    "            if \"avg10\" in col:\n",
    "                ax.plot(sys_df[\"t_s\"], pd.to_numeric(sys_df[col], errors=\"coerce\"),\n",
    "                        linewidth=0.8, label=col)\n",
    "        shade_requests(ax, wins, color=color)\n",
    "        ax.set_title(f\"{label} — PSI avg10\")\n",
    "        ax.set_xlabel(\"time (s)\")\n",
    "        ax.legend(fontsize=7)\n",
    "        ax.grid(True, alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No PSI columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5311d",
   "metadata": {},
   "source": [
    "## 10 — CPU frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_cols = sorted([c for c in n_sys.columns if c.endswith(\"_freq_khz\")])\n",
    "print(f\"CPU freq columns: {len(freq_cols)}\")\n",
    "\n",
    "if freq_cols:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "    for ax, sys_df, wins, label, color in [\n",
    "        (axes[0], n_sys, n_wins, \"neutral\", \"C0\"),\n",
    "        (axes[1], e_sys, e_wins, \"emotional\", \"C1\"),\n",
    "    ]:\n",
    "        for col in freq_cols:\n",
    "            freq_mhz = pd.to_numeric(sys_df[col], errors=\"coerce\") / 1000\n",
    "            ax.plot(sys_df[\"t_s\"], freq_mhz, linewidth=0.4, alpha=0.5)\n",
    "        # Plot mean across cores\n",
    "        mean_freq = sys_df[freq_cols].apply(pd.to_numeric, errors=\"coerce\").mean(axis=1) / 1000\n",
    "        ax.plot(sys_df[\"t_s\"], mean_freq, linewidth=1.5, color=\"black\", label=\"mean\")\n",
    "        shade_requests(ax, wins, color=color)\n",
    "        ax.set_title(f\"{label} — CPU freq\")\n",
    "        ax.set_xlabel(\"time (s)\")\n",
    "        ax.set_ylabel(\"MHz\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No CPU freq columns found (scaling_cur_freq may not be available).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b10626",
   "metadata": {},
   "source": [
    "## 11 — Network & Disk I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_cols = sorted([c for c in n_sys.columns if c.endswith((\"_rx_bytes\", \"_tx_bytes\"))])\n",
    "disk_cols = sorted([c for c in n_sys.columns if c.endswith((\"_reads_completed\", \"_writes_completed\"))])\n",
    "\n",
    "print(f\"Network columns: {net_cols}\")\n",
    "print(f\"Disk columns: {disk_cols}\")\n",
    "\n",
    "if net_cols:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    for ax, sys_df, label, color in [\n",
    "        (axes[0], n_sys, \"neutral\", \"C0\"),\n",
    "        (axes[1], e_sys, \"emotional\", \"C1\"),\n",
    "    ]:\n",
    "        for col in net_cols:\n",
    "            rate = pd.to_numeric(sys_df[col], errors=\"coerce\").diff() / sys_df[\"t_s\"].diff()\n",
    "            ax.plot(sys_df[\"t_s\"], rate / 1024, linewidth=0.8, label=col)\n",
    "        ax.set_title(f\"{label} — network KB/s\")\n",
    "        ax.set_xlabel(\"time (s)\")\n",
    "        ax.set_ylabel(\"KB/s\")\n",
    "        ax.legend(fontsize=6)\n",
    "        ax.grid(True, alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31eb7c3",
   "metadata": {},
   "source": [
    "## 12 — Process-level CPU (LLM container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4286f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "for ax, proc, wins, label, color in [\n",
    "    (axes[0], n_proc, n_wins, \"neutral\", \"C0\"),\n",
    "    (axes[1], e_proc, e_wins, \"emotional\", \"C1\"),\n",
    "]:\n",
    "    # CPU utilization: delta(utime+stime) / delta(total)\n",
    "    dt_total = proc[\"cpu_total_jiffies\"].diff()\n",
    "    dt_proc  = proc[\"proc_utime_jiffies\"].diff() + proc[\"proc_stime_jiffies\"].diff()\n",
    "    cpu_pct = (dt_proc / dt_total * 100).clip(0, 100)\n",
    "    ax.plot(proc[\"t_s\"], cpu_pct, linewidth=0.8, color=color)\n",
    "    shade_requests(ax, wins, color=color)\n",
    "    ax.set_title(f\"{label} — LLM container CPU %\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"CPU %\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d8854",
   "metadata": {},
   "source": [
    "## 13 — Quick summary\n",
    "\n",
    "Side-by-side descriptive stats for all perf events. Statistical tests will be added later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact summary table\n",
    "compact = summary[[c for c in summary.columns if \"mean\" in c or \"std\" in c or \"median\" in c or \"sum\" in c]]\n",
    "compact.style.format(\"{:.2f}\", na_rep=\"—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "# Set BASE_DIR to where you rsync'd the runs on your Mac.\n",
    "# Example: BASE_DIR = Path.home() / \"Desktop\" / \"mccviahat_runs\" / \"runs\"\n",
    "BASE_DIR = Path.home() / \"Desktop\" / \"mccviahat_runs\" / \"runs\"\n",
    "\n",
    "NEUTRAL_RUN   = \"2026-02-05T23-34-51_neutral\"\n",
    "EMOTIONAL_RUN = \"2026-02-05T23-37-41_emotional\"\n",
    "\n",
    "# Alignment: \"perf_start\" (simplest) or \"first_request\"\n",
    "ALIGN_MODE = \"first_request\"\n",
    "\n",
    "# Smoothing windows (in milliseconds)\n",
    "SMOOTH_MS_FAST = 50\n",
    "SMOOTH_MS_SLOW = 250\n",
    "\n",
    "def run_dir(name: str) -> Path:\n",
    "    p = BASE_DIR / name\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Run dir not found: {p}\")\n",
    "    return p\n",
    "\n",
    "n_dir = run_dir(NEUTRAL_RUN)\n",
    "e_dir = run_dir(EMOTIONAL_RUN)\n",
    "\n",
    "required = [\"perf_stat.txt\", \"kernel_log.txt\", \"collector_meta.json\", \"meta.json\", \"responses.jsonl\"]\n",
    "for d in [n_dir, e_dir]:\n",
    "    for f in required:\n",
    "        fp = d / f\n",
    "        assert fp.exists(), f\"Missing {fp}\"\n",
    "\n",
    "n_dir, e_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cebec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def load_requests(run_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for line in (run_dir / \"responses.jsonl\").read_text(encoding=\"utf-8\").splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "    df = pd.DataFrame(rows)\n",
    "    cols = [\"id\",\"title\",\"ok\",\"t_request_start_ns\",\"t_request_end_ns\"]\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df = df.sort_values(\"t_request_start_ns\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_t0_ns(meta: dict, collector_meta: dict) -> int:\n",
    "    # prefer collector start if present\n",
    "    # collector_meta has t0_ns; runner meta has t0_ns too\n",
    "    if \"t0_ns\" in collector_meta:\n",
    "        return int(collector_meta[\"t0_ns\"])\n",
    "    if \"t0_ns\" in meta:\n",
    "        return int(meta[\"t0_ns\"])\n",
    "    raise KeyError(\"No t0_ns found in meta/collector_meta\")\n",
    "\n",
    "n_meta = load_json(n_dir / \"meta.json\")\n",
    "e_meta = load_json(e_dir / \"meta.json\")\n",
    "n_cmeta = load_json(n_dir / \"collector_meta.json\")\n",
    "e_cmeta = load_json(e_dir / \"collector_meta.json\")\n",
    "\n",
    "n_req = load_requests(n_dir)\n",
    "e_req = load_requests(e_dir)\n",
    "\n",
    "n_t0 = get_t0_ns(n_meta, n_cmeta)\n",
    "e_t0 = get_t0_ns(e_meta, e_cmeta)\n",
    "\n",
    "print(\"Neutral requests:\", len(n_req), \"OK:\", bool(n_req.get(\"ok\", pd.Series([True])).all()))\n",
    "print(\"Emotional requests:\", len(e_req), \"OK:\", bool(e_req.get(\"ok\", pd.Series([True])).all()))\n",
    "print(\"Neutral perf events:\", len(n_cmeta.get(\"perf_events\", [])))\n",
    "print(\"Emotional perf events:\", len(e_cmeta.get(\"perf_events\", [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_perf_stat(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse perf stat -I output to tidy df: t_ms, event, value.\"\"\"\n",
    "    lines = path.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines()\n",
    "    data = []\n",
    "    # Example-ish: '  1.000123  1234  irq:softirq_entry'\n",
    "    # Sometimes value is '<not counted>' or '<not supported>'; keep NaN.\n",
    "    line_re = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)?)\\s+(\\S+)\\s+(.+?)\\s*$\")\n",
    "    for ln in lines:\n",
    "        if not ln.strip() or ln.lstrip().startswith(\"#\"):\n",
    "            continue\n",
    "        m = line_re.match(ln)\n",
    "        if not m:\n",
    "            continue\n",
    "        t_s = float(m.group(1))\n",
    "        raw = m.group(2).strip()\n",
    "        evt = m.group(3).strip()\n",
    "        raw_norm = raw.replace(\",\", \"\")\n",
    "        val = np.nan\n",
    "        if raw_norm.startswith(\"<\"):\n",
    "            val = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                val = float(raw_norm)\n",
    "            except ValueError:\n",
    "                val = np.nan\n",
    "        data.append((t_s * 1000.0, evt, val))\n",
    "    df = pd.DataFrame(data, columns=[\"t_ms\",\"event\",\"value\"])\n",
    "    return df\n",
    "\n",
    "def perf_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    w = df.pivot_table(index=\"t_ms\", columns=\"event\", values=\"value\", aggfunc=\"first\").sort_index()\n",
    "    w = w.reset_index()\n",
    "    w[\"t_s\"] = w[\"t_ms\"] / 1000.0\n",
    "    return w\n",
    "\n",
    "n_perf_tidy = parse_perf_stat(n_dir / \"perf_stat.txt\")\n",
    "e_perf_tidy = parse_perf_stat(e_dir / \"perf_stat.txt\")\n",
    "\n",
    "n_perf = perf_wide(n_perf_tidy)\n",
    "e_perf = perf_wide(e_perf_tidy)\n",
    "\n",
    "common_events = sorted(set(n_perf_tidy[\"event\"]) & set(e_perf_tidy[\"event\"]))\n",
    "missing_n = sorted(set(e_perf_tidy[\"event\"]) - set(n_perf_tidy[\"event\"]))\n",
    "missing_e = sorted(set(n_perf_tidy[\"event\"]) - set(e_perf_tidy[\"event\"]))\n",
    "\n",
    "print(\"Common events:\", len(common_events))\n",
    "if missing_n:\n",
    "    print(\"Events only in emotional (missing in neutral):\", missing_n[:10], \"...\" if len(missing_n)>10 else \"\")\n",
    "if missing_e:\n",
    "    print(\"Events only in neutral (missing in emotional):\", missing_e[:10], \"...\" if len(missing_e)>10 else \"\")\n",
    "common_events[:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_windows(req_df: pd.DataFrame, t0_ns: int) -> list[tuple[float,float]]:\n",
    "    wins = []\n",
    "    if req_df.empty:\n",
    "        return wins\n",
    "    for _, r in req_df.iterrows():\n",
    "        s = (int(r[\"t_request_start_ns\"]) - int(t0_ns)) / 1e9\n",
    "        e = (int(r[\"t_request_end_ns\"]) - int(t0_ns)) / 1e9\n",
    "        wins.append((s, e))\n",
    "    return wins\n",
    "\n",
    "n_wins = request_windows(n_req, n_t0)\n",
    "e_wins = request_windows(e_req, e_t0)\n",
    "\n",
    "def align_to_first_request(t_s: pd.Series, wins: list[tuple[float,float]]) -> pd.Series:\n",
    "    if not wins:\n",
    "        return t_s\n",
    "    shift = wins[0][0]\n",
    "    return t_s - shift\n",
    "\n",
    "# Align time axes if requested\n",
    "if ALIGN_MODE == \"first_request\":\n",
    "    n_perf[\"t_s_aligned\"] = align_to_first_request(n_perf[\"t_s\"], n_wins)\n",
    "    e_perf[\"t_s_aligned\"] = align_to_first_request(e_perf[\"t_s\"], e_wins)\n",
    "    # also shift windows for plotting\n",
    "    n_wins_aligned = [(s - n_wins[0][0], e - n_wins[0][0]) for s,e in n_wins] if n_wins else []\n",
    "    e_wins_aligned = [(s - e_wins[0][0], e - e_wins[0][0]) for s,e in e_wins] if e_wins else []\n",
    "else:\n",
    "    n_perf[\"t_s_aligned\"] = n_perf[\"t_s\"]\n",
    "    e_perf[\"t_s_aligned\"] = e_perf[\"t_s\"]\n",
    "    n_wins_aligned = n_wins\n",
    "    e_wins_aligned = e_wins\n",
    "\n",
    "print(\"ALIGN_MODE =\", ALIGN_MODE)\n",
    "print(\"Neutral first request window:\", n_wins[0] if n_wins else None)\n",
    "print(\"Emotional first request window:\", e_wins[0] if e_wins else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b77cf",
   "metadata": {},
   "source": [
    "## Figure 1: Run timeline overview (with request spans)\n",
    "\n",
    "We overlay one representative tracepoint (if present) to show where activity lines up with inference windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587616d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shade_windows(ax, wins, alpha=0.12):\n",
    "    for s,e in wins:\n",
    "        ax.axvspan(s, e, alpha=alpha)\n",
    "\n",
    "# choose a representative event if available\n",
    "candidates = [\"irq:softirq_entry\", \"irq:irq_handler_entry\", \"tlb:tlb_flush\"]\n",
    "rep = next((c for c in candidates if c in n_perf.columns and c in e_perf.columns), None)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "shade_windows(ax, n_wins_aligned, alpha=0.10)  # using neutral windows just for visual reference\n",
    "if rep:\n",
    "    ax.plot(n_perf[\"t_s_aligned\"], n_perf[rep], label=f\"neutral {rep}\")\n",
    "    ax.plot(e_perf[\"t_s_aligned\"], e_perf[rep], label=f\"emotional {rep}\")\n",
    "    ax.set_ylabel(\"count per 1ms bucket\")\n",
    "else:\n",
    "    ax.text(0.02, 0.9, \"No representative event found in common events.\", transform=ax.transAxes)\n",
    "ax.set_xlabel(\"time (s)\")\n",
    "ax.set_title(\"Timeline overview (request spans shaded)\")\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "rep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b133280",
   "metadata": {},
   "source": [
    "## Helper utilities for plotting and stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean_ms(series: pd.Series, t_ms: pd.Series, window_ms: int) -> pd.Series:\n",
    "    # approximate rolling window by converting to number of samples using median dt\n",
    "    dt = np.median(np.diff(t_ms.values)) if len(t_ms) > 2 else 1.0\n",
    "    w = max(1, int(round(window_ms / dt)))\n",
    "    return series.rolling(window=w, min_periods=max(1, w//4)).mean()\n",
    "\n",
    "def describe_series(x: pd.Series) -> dict:\n",
    "    x = x.dropna()\n",
    "    if x.empty:\n",
    "        return {\"n\": 0}\n",
    "    return {\n",
    "        \"n\": int(x.shape[0]),\n",
    "        \"mean\": float(x.mean()),\n",
    "        \"std\": float(x.std(ddof=1)) if x.shape[0] > 1 else 0.0,\n",
    "        \"p50\": float(x.quantile(0.50)),\n",
    "        \"p95\": float(x.quantile(0.95)),\n",
    "        \"p99\": float(x.quantile(0.99)),\n",
    "        \"cv\": float(x.std(ddof=1) / x.mean()) if x.mean() != 0 and x.shape[0] > 1 else np.nan,\n",
    "    }\n",
    "\n",
    "def plot_timeseries(evt: str, smooth_ms: int = SMOOTH_MS_FAST, ylabel: str = \"count per 1ms bucket\"):\n",
    "    if evt not in n_perf.columns or evt not in e_perf.columns:\n",
    "        print(f\"Missing event: {evt}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    n_sm = rolling_mean_ms(n_perf[evt], n_perf[\"t_ms\"], smooth_ms)\n",
    "    e_sm = rolling_mean_ms(e_perf[evt], e_perf[\"t_ms\"], smooth_ms)\n",
    "\n",
    "    ax.plot(n_perf[\"t_s_aligned\"], n_sm, label=\"neutral\")\n",
    "    ax.plot(e_perf[\"t_s_aligned\"], e_sm, label=\"emotional\")\n",
    "    shade_windows(ax, n_wins_aligned, alpha=0.08)\n",
    "    ax.set_title(f\"{evt} (smoothed {smooth_ms} ms)\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution(evt: str, bins: int = 80):\n",
    "    if evt not in n_perf.columns or evt not in e_perf.columns:\n",
    "        print(f\"Missing event: {evt}\")\n",
    "        return\n",
    "    n_x = n_perf[evt].dropna().values\n",
    "    e_x = e_perf[evt].dropna().values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.hist(n_x, bins=bins, density=True, alpha=0.5, label=\"neutral\")\n",
    "    ax.hist(e_x, bins=bins, density=True, alpha=0.5, label=\"emotional\")\n",
    "    ax.set_title(f\"{evt} distribution (per 1ms bucket)\")\n",
    "    ax.set_xlabel(\"value\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def summarize_evt(evt: str) -> pd.DataFrame:\n",
    "    if evt not in n_perf.columns or evt not in e_perf.columns:\n",
    "        return pd.DataFrame([{\"event\": evt, \"error\": \"missing\"}])\n",
    "    s = {\n",
    "        \"event\": evt,\n",
    "        **{f\"neutral_{k}\": v for k,v in describe_series(n_perf[evt]).items()},\n",
    "        **{f\"emotional_{k}\": v for k,v in describe_series(e_perf[evt]).items()},\n",
    "    }\n",
    "    return pd.DataFrame([s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f256e",
   "metadata": {},
   "source": [
    "## Interrupt activity (IRQ / softIRQ)\n",
    "\n",
    "These reflect asynchronous kernel activity. We compare smoothed time series and per-bucket distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b836e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "irq_events = [\n",
    "    \"irq:irq_handler_entry\",\n",
    "    \"irq:softirq_raise\",\n",
    "    \"irq:softirq_entry\",\n",
    "    \"irq:softirq_exit\",\n",
    "    \"irq:tasklet_entry\",\n",
    "    \"irq:tasklet_exit\",\n",
    "]\n",
    "irq_events = [e for e in irq_events if e in common_events]\n",
    "irq_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for evt in irq_events:\n",
    "    plot_timeseries(evt, smooth_ms=SMOOTH_MS_FAST)\n",
    "    plot_distribution(evt, bins=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bac926",
   "metadata": {},
   "source": [
    "## TLB flush activity (proxy for shootdowns)\n",
    "\n",
    "We plot the time series and an inter-arrival-time (IAT) distribution derived from bucketed counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399059cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TLB_EVT = \"tlb:tlb_flush\"\n",
    "assert TLB_EVT in common_events, \"tlb:tlb_flush not available in both runs\"\n",
    "\n",
    "plot_timeseries(TLB_EVT, smooth_ms=SMOOTH_MS_FAST)\n",
    "plot_distribution(TLB_EVT, bins=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccedeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_counts_to_event_times(t_s: np.ndarray, counts: np.ndarray) -> np.ndarray:\n",
    "    # Expand per-bucket counts into approximate event times at bucket centers.\n",
    "    # This is an approximation but sufficient for comparing IAT distributions.\n",
    "    times = []\n",
    "    for ts, c in zip(t_s, counts):\n",
    "        if np.isnan(c) or c <= 0:\n",
    "            continue\n",
    "        k = int(round(c))\n",
    "        times.extend([ts] * k)\n",
    "    return np.array(times, dtype=float)\n",
    "\n",
    "def plot_iat(evt: str, max_ms: int = 200, bins: int = 120):\n",
    "    n_times = bucket_counts_to_event_times(n_perf[\"t_s_aligned\"].values, n_perf[evt].values)\n",
    "    e_times = bucket_counts_to_event_times(e_perf[\"t_s_aligned\"].values, e_perf[evt].values)\n",
    "\n",
    "    n_iat = np.diff(np.sort(n_times)) * 1000.0  # ms\n",
    "    e_iat = np.diff(np.sort(e_times)) * 1000.0  # ms\n",
    "\n",
    "    n_iat = n_iat[(n_iat >= 0) & (n_iat <= max_ms)]\n",
    "    e_iat = e_iat[(e_iat >= 0) & (e_iat <= max_ms)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.hist(n_iat, bins=bins, density=True, alpha=0.5, label=\"neutral\")\n",
    "    ax.hist(e_iat, bins=bins, density=True, alpha=0.5, label=\"emotional\")\n",
    "    ax.set_title(f\"Inter-arrival times for {evt} (clipped to {max_ms} ms)\")\n",
    "    ax.set_xlabel(\"IAT (ms)\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Neutral IAT stats:\", describe_series(pd.Series(n_iat)))\n",
    "    print(\"Emotional IAT stats:\", describe_series(pd.Series(e_iat)))\n",
    "\n",
    "plot_iat(TLB_EVT, max_ms=200, bins=120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31703aa",
   "metadata": {},
   "source": [
    "## PMU jitter proxies\n",
    "\n",
    "We compute IPC (instructions per cycle) per 1ms bucket as a simple microarchitectural stability proxy.\n",
    "We compare time series (smoothed) and distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cycles and instructions columns (names differ)\n",
    "cycles_col = next((c for c in [\"cycles\", \"cpu-cycles\"] if c in n_perf.columns and c in e_perf.columns), None)\n",
    "inst_col   = next((c for c in [\"instructions\"] if c in n_perf.columns and c in e_perf.columns), None)\n",
    "\n",
    "print(\"cycles_col:\", cycles_col, \"inst_col:\", inst_col)\n",
    "\n",
    "assert cycles_col and inst_col, \"Need cycles and instructions in perf output.\"\n",
    "\n",
    "n_ipc = n_perf[inst_col] / n_perf[cycles_col]\n",
    "e_ipc = e_perf[inst_col] / e_perf[cycles_col]\n",
    "\n",
    "n_perf[\"ipc\"] = n_ipc.replace([np.inf, -np.inf], np.nan)\n",
    "e_perf[\"ipc\"] = e_ipc.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "plot_timeseries(\"ipc\", smooth_ms=SMOOTH_MS_FAST, ylabel=\"IPC\")\n",
    "plot_distribution(\"ipc\", bins=100)\n",
    "\n",
    "print(\"IPC stats neutral:\", describe_series(n_perf[\"ipc\"]))\n",
    "print(\"IPC stats emotional:\", describe_series(e_perf[\"ipc\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f397a",
   "metadata": {},
   "source": [
    "## Power / thermal / throttling\n",
    "\n",
    "Energy counters are treated as counters; we take first differences and then smooth heavily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203179b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_rate(series: pd.Series) -> pd.Series:\n",
    "    # perf -I can report per-interval counts or cumulative depending on event; for energy it is often cumulative.\n",
    "    # Taking diff is safe for comparing shape; negative diffs become NaN.\n",
    "    d = series.diff()\n",
    "    d[d < 0] = np.nan\n",
    "    return d\n",
    "\n",
    "energy_events = [c for c in common_events if \"energy\" in c]\n",
    "throttle_evt = \"core_power.throttle\" if \"core_power.throttle\" in common_events else None\n",
    "thermal_evt = \"msr/cpu_thermal_margin/\" if \"msr/cpu_thermal_margin/\" in common_events else None\n",
    "\n",
    "energy_events, throttle_evt, thermal_evt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06630be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy plots\n",
    "for evt in energy_events:\n",
    "    n_perf[f\"{evt}__d\"] = energy_rate(n_perf[evt])\n",
    "    e_perf[f\"{evt}__d\"] = energy_rate(e_perf[evt])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    n_sm = rolling_mean_ms(n_perf[f\"{evt}__d\"], n_perf[\"t_ms\"], SMOOTH_MS_SLOW)\n",
    "    e_sm = rolling_mean_ms(e_perf[f\"{evt}__d\"], e_perf[\"t_ms\"], SMOOTH_MS_SLOW)\n",
    "    ax.plot(n_perf[\"t_s_aligned\"], n_sm, label=\"neutral\")\n",
    "    ax.plot(e_perf[\"t_s_aligned\"], e_sm, label=\"emotional\")\n",
    "    shade_windows(ax, n_wins_aligned, alpha=0.08)\n",
    "    ax.set_title(f\"{evt} delta per 1ms bucket (smoothed {SMOOTH_MS_SLOW} ms)\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"delta\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# Throttle\n",
    "if throttle_evt:\n",
    "    plot_timeseries(throttle_evt, smooth_ms=SMOOTH_MS_SLOW)\n",
    "    plot_distribution(throttle_evt, bins=80)\n",
    "\n",
    "# Thermal margin (slow)\n",
    "if thermal_evt:\n",
    "    plot_timeseries(thermal_evt, smooth_ms=SMOOTH_MS_SLOW, ylabel=\"thermal margin (units as reported)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6682f35",
   "metadata": {},
   "source": [
    "## Kernel log scan (MCE, hardware errors, throttling messages)\n",
    "\n",
    "We extract lines matching key patterns. If empty, that is a valid outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_kernel_log(path: Path, patterns: str = r\"mce|machine check|hardware error|edac|thermal|throttle|watchdog\") -> pd.DataFrame:\n",
    "    rx = re.compile(patterns, flags=re.IGNORECASE)\n",
    "    lines = path.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines()\n",
    "    hits = [ln for ln in lines if rx.search(ln)]\n",
    "    return pd.DataFrame({\"matches\": hits})\n",
    "\n",
    "n_hits = scan_kernel_log(n_dir / \"kernel_log.txt\")\n",
    "e_hits = scan_kernel_log(e_dir / \"kernel_log.txt\")\n",
    "\n",
    "print(\"Neutral kernel matches:\", len(n_hits))\n",
    "print(\"Emotional kernel matches:\", len(e_hits))\n",
    "\n",
    "display(n_hits.head(20))\n",
    "display(e_hits.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e2d98",
   "metadata": {},
   "source": [
    "## Summary table\n",
    "\n",
    "This produces a compact comparison table for the signals you care about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "\n",
    "# Interrupts + TLB\n",
    "for evt in (irq_events + [TLB_EVT]):\n",
    "    if evt in n_perf.columns and evt in e_perf.columns:\n",
    "        summary_rows.append(summarize_evt(evt).iloc[0].to_dict())\n",
    "\n",
    "# IPC\n",
    "summary_rows.append(summarize_evt(\"ipc\").iloc[0].to_dict())\n",
    "\n",
    "# Throttle\n",
    "if throttle_evt:\n",
    "    summary_rows.append(summarize_evt(throttle_evt).iloc[0].to_dict())\n",
    "\n",
    "# Energy deltas (use derived columns)\n",
    "for evt in energy_events:\n",
    "    col = f\"{evt}__d\"\n",
    "    if col in n_perf.columns and col in e_perf.columns:\n",
    "        # temporarily create standard names for summarizer\n",
    "        n_perf[col] = n_perf[col]\n",
    "        e_perf[col] = e_perf[col]\n",
    "        # manual describe\n",
    "        s = {\"event\": col}\n",
    "        s.update({f\"neutral_{k}\": v for k,v in describe_series(n_perf[col]).items()})\n",
    "        s.update({f\"emotional_{k}\": v for k,v in describe_series(e_perf[col]).items()})\n",
    "        summary_rows.append(s)\n",
    "\n",
    "summary = pd.DataFrame(summary_rows)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: export all current figures to disk (uncomment if desired)\n",
    "# out_fig = Path.cwd() / \"figures\" / f\"{NEUTRAL_RUN}__vs__{EMOTIONAL_RUN}\"\n",
    "# out_fig.mkdir(parents=True, exist_ok=True)\n",
    "# for i, fig_num in enumerate(plt.get_fignums(), start=1):\n",
    "#     fig = plt.figure(fig_num)\n",
    "#     fig.savefig(out_fig / f\"fig_{i:02d}.png\", bbox_inches=\"tight\")\n",
    "# print(\"Saved to:\", out_fig)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
