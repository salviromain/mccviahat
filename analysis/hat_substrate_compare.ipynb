{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAT Substrate Comparison - Neutral vs Emotional\n",
    "\n",
    "This notebook loads and explores the data collected by `substrate_collector.py`\n",
    "during two LLM inference runs (one per prompt condition) on a CloudLab bare-metal node.\n",
    "\n",
    "**Data sources per run:**\n",
    "\n",
    "| File | Resolution | Content |\n",
    "|------|------------|---------|\n",
    "| `perf_stat.txt` | 1 ms buckets | 16 system-wide perf events (IRQs, softirqs, TLB, power) |\n",
    "| `proc_sample.csv` | 200 ms | Per-process CPU time + RSS for the llama.cpp container PID |\n",
    "| `proc_system_sample.csv` | 200 ms | /proc/interrupts, /proc/softirqs, PSI, net, disk, freq |\n",
    "| `responses.jsonl` | per request | LLM request timing + llama.cpp internal metrics |\n",
    "| `collector_meta.json` | once | Collector config, t0/t1 timestamps |\n",
    "\n",
    "**Timing model:** Each run records a fixed **44 s** window\n",
    "(2 s baseline + 40 s prompt budget + 2 s tail) so the time axes are directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['figure.figsize'] = (12, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Configuration\n",
    "\n",
    "Point `BASE_DIR` at the folder that contains your run directories.\n",
    "Each run directory name looks like `2026-02-09T22-18-50_emotional`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.home() / 'Desktop' / 'mccviahat' / 'mccviahat_runs'\n",
    "\n",
    "NEUTRAL_RUN   = '2026-02-09T22-28-54_neutral'\n",
    "EMOTIONAL_RUN = '2026-02-09T22-18-50_emotional'\n",
    "\n",
    "n_dir = BASE_DIR / NEUTRAL_RUN\n",
    "e_dir = BASE_DIR / EMOTIONAL_RUN\n",
    "\n",
    "for d in [n_dir, e_dir]:\n",
    "    assert d.exists(), f'Missing: {d}'\n",
    "print('Both run directories found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Parsers\n",
    "\n",
    "### perf stat parser\n",
    "\n",
    "Each line in `perf_stat.txt` follows this format (the `#` comment is always present):\n",
    "```\n",
    "<timestamp>  <value>            <event_name>             # <rate> /sec\n",
    "<timestamp>  <value> <unit>     <event_name>             # <rate> <desc>\n",
    "```\n",
    "\n",
    "The event name is always the **last token before the `#`**.  This makes parsing robust\n",
    "regardless of whether a unit (`msec`, `Joules`, `C`) is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_perf_stat(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse perf stat -I output into tidy (t_s, event, value) rows.\n",
    "    \n",
    "    Strategy: split each line on '#', take the left side,\n",
    "    the event name is the LAST token before '#'.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding='utf-8', errors='replace').splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "        parts = line.split('#', 1)\n",
    "        left = parts[0].strip()\n",
    "        tokens = left.split()\n",
    "        if len(tokens) < 3:\n",
    "            continue\n",
    "        \n",
    "        t_s_str = tokens[0]\n",
    "        event   = tokens[-1]       # last token before '#'\n",
    "        raw_val = tokens[1]        # second token is the count/value\n",
    "        \n",
    "        try:\n",
    "            t_s = float(t_s_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        raw_val = raw_val.replace(',', '')\n",
    "        if raw_val.startswith('<'):\n",
    "            val = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                val = float(raw_val)\n",
    "            except ValueError:\n",
    "                val = np.nan\n",
    "        \n",
    "        rows.append((t_s, event, val))\n",
    "    \n",
    "    return pd.DataFrame(rows, columns=['t_s', 'event', 'value'])\n",
    "\n",
    "\n",
    "def perf_to_wide(tidy: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Pivot tidy perf -> wide: one row per timestamp, one column per event.\"\"\"\n",
    "    w = tidy.pivot_table(index='t_s', columns='event', values='value', aggfunc='first')\n",
    "    return w.sort_index().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response and /proc loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text(encoding='utf-8'))\n",
    "\n",
    "\n",
    "def load_responses(run_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load responses.jsonl and extract llama.cpp timings.\"\"\"\n",
    "    rows = []\n",
    "    for line in (run_dir / 'responses.jsonl').read_text(encoding='utf-8').splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        r = json.loads(line)\n",
    "        try:\n",
    "            resp = json.loads(r['response_raw'])\n",
    "            timings = resp.get('timings', {})\n",
    "            r['prompt_ms']         = timings.get('prompt_ms')\n",
    "            r['predicted_ms']      = timings.get('predicted_ms')\n",
    "            r['tokens_evaluated']  = resp.get('tokens_evaluated')\n",
    "            r['tokens_predicted']  = resp.get('tokens_predicted')\n",
    "        except Exception:\n",
    "            pass\n",
    "        rows.append(r)\n",
    "    df = pd.DataFrame(rows)\n",
    "    keep = [c for c in ['id', 'title', 'ok', 't_request_start_ns', 't_request_end_ns',\n",
    "                         'prompt_ms', 'predicted_ms', 'tokens_evaluated', 'tokens_predicted']\n",
    "            if c in df.columns]\n",
    "    df = df[keep].sort_values('t_request_start_ns').reset_index(drop=True)\n",
    "    df['duration_s'] = (df['t_request_end_ns'] - df['t_request_start_ns']) / 1e9\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_proc_system(run_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load proc_system_sample.csv and add relative time.\"\"\"\n",
    "    df = pd.read_csv(run_dir / 'proc_system_sample.csv')\n",
    "    df['t_s'] = (df['timestamp_ns'] - df['timestamp_ns'].iloc[0]) / 1e9\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_proc_sample(run_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load proc_sample.csv (per-process stats) and add relative time.\"\"\"\n",
    "    df = pd.read_csv(run_dir / 'proc_sample.csv')\n",
    "    df['t_s'] = (df['timestamp_ns'] - df['timestamp_ns'].iloc[0]) / 1e9\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cmeta = load_json(n_dir / 'collector_meta.json')\n",
    "e_cmeta = load_json(e_dir / 'collector_meta.json')\n",
    "\n",
    "n_resp = load_responses(n_dir)\n",
    "e_resp = load_responses(e_dir)\n",
    "\n",
    "n_perf_tidy = parse_perf_stat(n_dir / 'perf_stat.txt')\n",
    "e_perf_tidy = parse_perf_stat(e_dir / 'perf_stat.txt')\n",
    "n_perf = perf_to_wide(n_perf_tidy)\n",
    "e_perf = perf_to_wide(e_perf_tidy)\n",
    "\n",
    "n_sys = load_proc_system(n_dir)\n",
    "e_sys = load_proc_system(e_dir)\n",
    "\n",
    "n_proc = load_proc_sample(n_dir)\n",
    "e_proc = load_proc_sample(e_dir)\n",
    "\n",
    "# Sanity report\n",
    "perf_events = sorted(n_perf_tidy['event'].unique())\n",
    "print(f'Perf events found ({len(perf_events)}): {perf_events}')\n",
    "print(f'Perf rows   - neutral: {len(n_perf):,}  emotional: {len(e_perf):,}')\n",
    "print(f'Proc system - neutral: {len(n_sys)}  emotional: {len(e_sys)}')\n",
    "print(f'Responses   - neutral: {len(n_resp)}/5  emotional: {len(e_resp)}/5')\n",
    "if len(e_resp) < 5:\n",
    "    print(f'  -> Emotional has {len(e_resp)}/5 (prompt budget ran out - expected for long prompts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Request timing comparison\n",
    "\n",
    "Emotional prompts are **longer** (more tokens to evaluate) so they take more time.\n",
    "With `n_predict=40`, the generation (predicted) time is similar, but prompt processing\n",
    "dominates for emotional prompts (~6 s vs ~0.15 s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cols = ['id', 'title', 'duration_s', 'tokens_evaluated', 'tokens_predicted', 'prompt_ms', 'predicted_ms']\n",
    "print('=== Neutral ===')\n",
    "display(n_resp[show_cols])\n",
    "print(f'\\n=== Emotional ({len(e_resp)}/5 completed) ===')\n",
    "display(e_resp[show_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Perf event summary\n",
    "\n",
    "Mean event counts per 1 ms bucket, comparing the two conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_events = sorted(set(n_perf.columns) & set(e_perf.columns) - {'t_s'})\n",
    "print(f'Events in both runs ({len(common_events)}):\\n')\n",
    "print(f'{\"Event\":35s}  {\"Neutral mean\":>14s}  {\"Emotional mean\":>14s}')\n",
    "print('-' * 68)\n",
    "for e in common_events:\n",
    "    print(f'{e:35s}  {n_perf[e].mean():14.2f}  {e_perf[e].mean():14.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Time-series plots\n",
    "\n",
    "Smoothed (500 ms rolling mean) time-series of key perf events.\n",
    "Shaded bands mark request windows; dotted line marks the end of the 2 s baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_windows(resp, t0_ns):\n",
    "    return [(float(r['t_request_start_ns'] - t0_ns) / 1e9,\n",
    "             float(r['t_request_end_ns']   - t0_ns) / 1e9)\n",
    "            for _, r in resp.iterrows()]\n",
    "\n",
    "n_wins = request_windows(n_resp, n_cmeta['t0_ns'])\n",
    "e_wins = request_windows(e_resp, e_cmeta['t0_ns'])\n",
    "\n",
    "\n",
    "def plot_event(evt, smooth_ms=500):\n",
    "    if evt not in n_perf.columns or evt not in e_perf.columns:\n",
    "        print(f'  {evt} not in both runs - skipping.')\n",
    "        return\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 3.5), sharey=True)\n",
    "    for ax, label, df, wins, color in [\n",
    "        (axes[0], 'Neutral',   n_perf, n_wins, 'steelblue'),\n",
    "        (axes[1], 'Emotional', e_perf, e_wins, 'firebrick'),\n",
    "    ]:\n",
    "        smoothed = df[evt].rolling(smooth_ms, min_periods=1).mean()\n",
    "        ax.plot(df['t_s'], smoothed, color=color, lw=0.8, alpha=0.9)\n",
    "        for s, e in wins:\n",
    "            ax.axvspan(s, e, alpha=0.10, color=color)\n",
    "        ax.axvline(2.0, ls=':', color='gray', lw=0.5)\n",
    "        ax.set_title(f'{label} - {evt}')\n",
    "        ax.set_xlabel('time (s)')\n",
    "    axes[0].set_ylabel(f'rolling {smooth_ms}ms mean')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evt in ['irq:irq_handler_entry', 'irq:softirq_entry', 'tlb:tlb_flush',\n",
    "            'context-switches', 'cpu-clock']:\n",
    "    plot_event(evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evt in ['power/energy-pkg/', 'power/energy-ram/', 'msr/cpu_thermal_margin/', 'core_power.throttle']:\n",
    "    plot_event(evt, smooth_ms=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Per-process CPU usage (from /proc)\n",
    "\n",
    "`proc_sample.csv` tracks the llama.cpp container's user + system CPU jiffies at 200 ms.\n",
    "We diff consecutive samples to approximate CPU utilization per interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 3.5), sharey=True)\n",
    "for ax, label, df, color in [\n",
    "    (axes[0], 'Neutral',   n_proc, 'steelblue'),\n",
    "    (axes[1], 'Emotional', e_proc, 'firebrick'),\n",
    "]:\n",
    "    cpu = (df['proc_utime_jiffies'] + df['proc_stime_jiffies']).diff()\n",
    "    ax.plot(df['t_s'], cpu, color=color, lw=0.8)\n",
    "    ax.axvline(2.0, ls=':', color='gray', lw=0.5)\n",
    "    ax.set_title(f'{label} - container CPU delta (jiffies/200ms)')\n",
    "    ax.set_xlabel('time (s)')\n",
    "axes[0].set_ylabel('d(utime+stime) jiffies')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - /proc softirq rates\n",
    "\n",
    "`proc_system_sample.csv` has cumulative `/proc/softirqs` counters.\n",
    "Diffing gives approximate rates per 200 ms interval.\n",
    "\n",
    "**Known issue (Feb 9 data):** PSI columns have duplicate names (cpu/memory/io all\n",
    "share `full_avg10`, etc.). Also, the collector had an `or`-chain bug that dropped\n",
    "values equal to 0.  Both are now fixed for the next run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softirq_names = ['HI', 'TIMER', 'NET_TX', 'NET_RX', 'BLOCK', 'TASKLET', 'SCHED', 'HRTIMER', 'RCU']\n",
    "available_sirqs = [s for s in softirq_names if s in n_sys.columns]\n",
    "\n",
    "if available_sirqs:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "    for ax, label, df, color in [\n",
    "        (axes[0], 'Neutral',   n_sys, 'steelblue'),\n",
    "        (axes[1], 'Emotional', e_sys, 'firebrick'),\n",
    "    ]:\n",
    "        for sirq in available_sirqs:\n",
    "            rate = df[sirq].diff() / df['t_s'].diff()\n",
    "            ax.plot(df['t_s'], rate, lw=0.8, alpha=0.7, label=sirq)\n",
    "        ax.set_title(f'{label} - softirq rates (/proc/softirqs)')\n",
    "        ax.set_xlabel('time (s)')\n",
    "        ax.legend(fontsize=7, ncol=2)\n",
    "    axes[0].set_ylabel('events/sec')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No softirq columns found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Data quality diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('DATA QUALITY SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "# 1. Perf events\n",
    "n_events = set(n_perf_tidy['event'].unique())\n",
    "e_events = set(e_perf_tidy['event'].unique())\n",
    "print(f'\\nPerf events: {len(n_events)} neutral, {len(e_events)} emotional')\n",
    "only_n = n_events - e_events\n",
    "only_e = e_events - n_events\n",
    "if only_n: print(f'  Only in neutral: {only_n}')\n",
    "if only_e: print(f'  Only in emotional: {only_e}')\n",
    "\n",
    "# 2. All-zero events\n",
    "print('\\nAll-zero perf events (recorded but never fired):')\n",
    "any_zero = False\n",
    "for evt in common_events:\n",
    "    n_sum, e_sum = n_perf[evt].sum(), e_perf[evt].sum()\n",
    "    if n_sum == 0 and e_sum == 0:\n",
    "        print(f'  {evt} - zero in both')\n",
    "        any_zero = True\n",
    "    elif n_sum == 0 or e_sum == 0:\n",
    "        print(f'  {evt} - zero in {\"neutral\" if n_sum == 0 else \"emotional\"} only')\n",
    "        any_zero = True\n",
    "if not any_zero:\n",
    "    print('  (none)')\n",
    "\n",
    "# 3. Responses\n",
    "print(f'\\nResponses: neutral={len(n_resp)}/5, emotional={len(e_resp)}/5')\n",
    "if len(e_resp) < 5:\n",
    "    print('  Emotional prompts are ~2x longer (~400 vs ~220 tokens).')\n",
    "    print('  At ~11s per prompt, only 4 fit in the 40s budget. This is expected.')\n",
    "    print('  Consider: increase budget_s to 60s, or reduce n_predict.')\n",
    "\n",
    "# 4. Duplicate column names in proc_system_sample (PSI bug)\n",
    "dup_cols = n_sys.columns[n_sys.columns.duplicated()].tolist()\n",
    "if dup_cols:\n",
    "    print(f'\\nDuplicate columns in proc_system_sample: {set(dup_cols)}')\n",
    "    print('  -> PSI columns for cpu/memory/io share names. Fixed for next run.')\n",
    "\n",
    "# 5. proc_system NaN check\n",
    "n_nans = n_sys.isna().sum()\n",
    "problem_cols = n_nans[n_nans > 0]\n",
    "if len(problem_cols) > 0:\n",
    "    print(f'\\nproc_system_sample NaN columns ({len(problem_cols)}):')\n",
    "    for c, cnt in problem_cols.items():\n",
    "        print(f'  {c}: {cnt}/{len(n_sys)} NaN')\n",
    "else:\n",
    "    print('\\nproc_system_sample: no NaN values')\n",
    "\n",
    "print('\\n' + '=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "This notebook is for **exploration** only. Once you have multiple runs per condition\n",
    "(replications), the statistical analysis will:\n",
    "\n",
    "1. Aggregate per-run summaries (mean event rate during inference window)\n",
    "2. Compare conditions with Mann-Whitney U / permutation tests\n",
    "3. Compute effect sizes (Cohen's d, rank-biserial correlation)\n",
    "4. Apply multiple-comparison correction (Holm-Bonferroni)\n",
    "\n",
    "**Action items from this exploration:**\n",
    "- Re-run with the fixed collector (PSI prefixes + or-chain fix)\n",
    "- Consider increasing `budget_s` to 60 s so all 5 emotional prompts complete\n",
    "- Collect >= 10 replications per condition for meaningful statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
