{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2112ff6c",
   "metadata": {},
   "source": [
    "# HAT Substrate Comparison: Neutral vs Emotional\n",
    "\n",
    "This notebook compares **system-wide substrate signals** collected per run:\n",
    "- `perf_stat.txt` (1 ms buckets from `sudo perf stat -I 1 ...`)\n",
    "- `kernel_log.txt` (kernel log slice for the run window)\n",
    "- `responses.jsonl` (request timing + success)\n",
    "\n",
    "It intentionally **does not** treat `/proc` metrics as evidence, except for optional alignment/sanity.\n",
    "\n",
    "You will set:\n",
    "- `BASE_DIR`\n",
    "- `NEUTRAL_RUN`\n",
    "- `EMOTIONAL_RUN`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "# Set BASE_DIR to where you rsync'd the runs on your Mac.\n",
    "# Example: BASE_DIR = Path.home() / \"Desktop\" / \"mccviahat_runs\" / \"runs\"\n",
    "BASE_DIR = Path.home() / \"Desktop\" / \"mccviahat_runs\" / \"runs\"\n",
    "\n",
    "NEUTRAL_RUN   = \"2026-02-05T23-34-51_neutral\"\n",
    "EMOTIONAL_RUN = \"2026-02-05T23-37-41_emotional\"\n",
    "\n",
    "# Alignment: \"perf_start\" (simplest) or \"first_request\"\n",
    "ALIGN_MODE = \"first_request\"\n",
    "\n",
    "# Smoothing windows (in milliseconds)\n",
    "SMOOTH_MS_FAST = 50\n",
    "SMOOTH_MS_SLOW = 250\n",
    "\n",
    "def run_dir(name: str) -> Path:\n",
    "    p = BASE_DIR / name\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Run dir not found: {p}\")\n",
    "    return p\n",
    "\n",
    "n_dir = run_dir(NEUTRAL_RUN)\n",
    "e_dir = run_dir(EMOTIONAL_RUN)\n",
    "\n",
    "required = [\"perf_stat.txt\", \"kernel_log.txt\", \"collector_meta.json\", \"meta.json\", \"responses.jsonl\"]\n",
    "for d in [n_dir, e_dir]:\n",
    "    for f in required:\n",
    "        fp = d / f\n",
    "        assert fp.exists(), f\"Missing {fp}\"\n",
    "\n",
    "n_dir, e_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cebec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def load_requests(run_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for line in (run_dir / \"responses.jsonl\").read_text(encoding=\"utf-8\").splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "    df = pd.DataFrame(rows)\n",
    "    cols = [\"id\",\"title\",\"ok\",\"t_request_start_ns\",\"t_request_end_ns\"]\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df = df.sort_values(\"t_request_start_ns\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_t0_ns(meta: dict, collector_meta: dict) -> int:\n",
    "    # prefer collector start if present\n",
    "    # collector_meta has t0_ns; runner meta has t0_ns too\n",
    "    if \"t0_ns\" in collector_meta:\n",
    "        return int(collector_meta[\"t0_ns\"])\n",
    "    if \"t0_ns\" in meta:\n",
    "        return int(meta[\"t0_ns\"])\n",
    "    raise KeyError(\"No t0_ns found in meta/collector_meta\")\n",
    "\n",
    "n_meta = load_json(n_dir / \"meta.json\")\n",
    "e_meta = load_json(e_dir / \"meta.json\")\n",
    "n_cmeta = load_json(n_dir / \"collector_meta.json\")\n",
    "e_cmeta = load_json(e_dir / \"collector_meta.json\")\n",
    "\n",
    "n_req = load_requests(n_dir)\n",
    "e_req = load_requests(e_dir)\n",
    "\n",
    "n_t0 = get_t0_ns(n_meta, n_cmeta)\n",
    "e_t0 = get_t0_ns(e_meta, e_cmeta)\n",
    "\n",
    "print(\"Neutral requests:\", len(n_req), \"OK:\", bool(n_req.get(\"ok\", pd.Series([True])).all()))\n",
    "print(\"Emotional requests:\", len(e_req), \"OK:\", bool(e_req.get(\"ok\", pd.Series([True])).all()))\n",
    "print(\"Neutral perf events:\", len(n_cmeta.get(\"perf_events\", [])))\n",
    "print(\"Emotional perf events:\", len(e_cmeta.get(\"perf_events\", [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_perf_stat(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse perf stat -I output to tidy df: t_ms, event, value.\"\"\"\n",
    "    lines = path.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines()\n",
    "    data = []\n",
    "    # Example-ish: '  1.000123  1234  irq:softirq_entry'\n",
    "    # Sometimes value is '<not counted>' or '<not supported>'; keep NaN.\n",
    "    line_re = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)?)\\s+(\\S+)\\s+(.+?)\\s*$\")\n",
    "    for ln in lines:\n",
    "        if not ln.strip() or ln.lstrip().startswith(\"#\"):\n",
    "            continue\n",
    "        m = line_re.match(ln)\n",
    "        if not m:\n",
    "            continue\n",
    "        t_s = float(m.group(1))\n",
    "        raw = m.group(2).strip()\n",
    "        evt = m.group(3).strip()\n",
    "        raw_norm = raw.replace(\",\", \"\")\n",
    "        val = np.nan\n",
    "        if raw_norm.startswith(\"<\"):\n",
    "            val = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                val = float(raw_norm)\n",
    "            except ValueError:\n",
    "                val = np.nan\n",
    "        data.append((t_s * 1000.0, evt, val))\n",
    "    df = pd.DataFrame(data, columns=[\"t_ms\",\"event\",\"value\"])\n",
    "    return df\n",
    "\n",
    "def perf_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    w = df.pivot_table(index=\"t_ms\", columns=\"event\", values=\"value\", aggfunc=\"first\").sort_index()\n",
    "    w = w.reset_index()\n",
    "    w[\"t_s\"] = w[\"t_ms\"] / 1000.0\n",
    "    return w\n",
    "\n",
    "n_perf_tidy = parse_perf_stat(n_dir / \"perf_stat.txt\")\n",
    "e_perf_tidy = parse_perf_stat(e_dir / \"perf_stat.txt\")\n",
    "\n",
    "n_perf = perf_wide(n_perf_tidy)\n",
    "e_perf = perf_wide(e_perf_tidy)\n",
    "\n",
    "common_events = sorted(set(n_perf_tidy[\"event\"]) & set(e_perf_tidy[\"event\"]))\n",
    "missing_n = sorted(set(e_perf_tidy[\"event\"]) - set(n_perf_tidy[\"event\"]))\n",
    "missing_e = sorted(set(n_perf_tidy[\"event\"]) - set(e_perf_tidy[\"event\"]))\n",
    "\n",
    "print(\"Common events:\", len(common_events))\n",
    "if missing_n:\n",
    "    print(\"Events only in emotional (missing in neutral):\", missing_n[:10], \"...\" if len(missing_n)>10 else \"\")\n",
    "if missing_e:\n",
    "    print(\"Events only in neutral (missing in emotional):\", missing_e[:10], \"...\" if len(missing_e)>10 else \"\")\n",
    "common_events[:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_windows(req_df: pd.DataFrame, t0_ns: int) -> list[tuple[float,float]]:\n",
    "    wins = []\n",
    "    if req_df.empty:\n",
    "        return wins\n",
    "    for _, r in req_df.iterrows():\n",
    "        s = (int(r[\"t_request_start_ns\"]) - int(t0_ns)) / 1e9\n",
    "        e = (int(r[\"t_request_end_ns\"]) - int(t0_ns)) / 1e9\n",
    "        wins.append((s, e))\n",
    "    return wins\n",
    "\n",
    "n_wins = request_windows(n_req, n_t0)\n",
    "e_wins = request_windows(e_req, e_t0)\n",
    "\n",
    "def align_to_first_request(t_s: pd.Series, wins: list[tuple[float,float]]) -> pd.Series:\n",
    "    if not wins:\n",
    "        return t_s\n",
    "    shift = wins[0][0]\n",
    "    return t_s - shift\n",
    "\n",
    "# Align time axes if requested\n",
    "if ALIGN_MODE == \"first_request\":\n",
    "    n_perf[\"t_s_aligned\"] = align_to_first_request(n_perf[\"t_s\"], n_wins)\n",
    "    e_perf[\"t_s_aligned\"] = align_to_first_request(e_perf[\"t_s\"], e_wins)\n",
    "    # also shift windows for plotting\n",
    "    n_wins_aligned = [(s - n_wins[0][0], e - n_wins[0][0]) for s,e in n_wins] if n_wins else []\n",
    "    e_wins_aligned = [(s - e_wins[0][0], e - e_wins[0][0]) for s,e in e_wins] if e_wins else []\n",
    "else:\n",
    "    n_perf[\"t_s_aligned\"] = n_perf[\"t_s\"]\n",
    "    e_perf[\"t_s_aligned\"] = e_perf[\"t_s\"]\n",
    "    n_wins_aligned = n_wins\n",
    "    e_wins_aligned = e_wins\n",
    "\n",
    "print(\"ALIGN_MODE =\", ALIGN_MODE)\n",
    "print(\"Neutral first request window:\", n_wins[0] if n_wins else None)\n",
    "print(\"Emotional first request window:\", e_wins[0] if e_wins else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b77cf",
   "metadata": {},
   "source": [
    "## Figure 1: Run timeline overview (with request spans)\n",
    "\n",
    "We overlay one representative tracepoint (if present) to show where activity lines up with inference windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587616d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shade_windows(ax, wins, alpha=0.12):\n",
    "    for s,e in wins:\n",
    "        ax.axvspan(s, e, alpha=alpha)\n",
    "\n",
    "# choose a representative event if available\n",
    "candidates = [\"irq:softirq_entry\", \"irq:irq_handler_entry\", \"tlb:tlb_flush\"]\n",
    "rep = next((c for c in candidates if c in n_perf.columns and c in e_perf.columns), None)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "shade_windows(ax, n_wins_aligned, alpha=0.10)  # using neutral windows just for visual reference\n",
    "if rep:\n",
    "    ax.plot(n_perf[\"t_s_aligned\"], n_perf[rep], label=f\"neutral {rep}\")\n",
    "    ax.plot(e_perf[\"t_s_aligned\"], e_perf[rep], label=f\"emotional {rep}\")\n",
    "    ax.set_ylabel(\"count per 1ms bucket\")\n",
    "else:\n",
    "    ax.text(0.02, 0.9, \"No representative event found in common events.\", transform=ax.transAxes)\n",
    "ax.set_xlabel(\"time (s)\")\n",
    "ax.set_title(\"Timeline overview (request spans shaded)\")\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "rep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b133280",
   "metadata": {},
   "source": [
    "## Helper utilities for plotting and stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean_ms(series: pd.Series, t_ms: pd.Series, window_ms: int) -> pd.Series:\n",
    "    # approximate rolling window by converting to number of samples using median dt\n",
    "    dt = np.median(np.diff(t_ms.values)) if len(t_ms) > 2 else 1.0\n",
    "    w = max(1, int(round(window_ms / dt)))\n",
    "    return series.rolling(window=w, min_periods=max(1, w//4)).mean()\n",
    "\n",
    "def describe_series(x: pd.Series) -> dict:\n",
    "    x = x.dropna()\n",
    "    if x.empty:\n",
    "        return {\"n\": 0}\n",
    "    return {\n",
    "        \"n\": int(x.shape[0]),\n",
    "        \"mean\": float(x.mean()),\n",
    "        \"std\": float(x.std(ddof=1)) if x.shape[0] > 1 else 0.0,\n",
    "        \"p50\": float(x.quantile(0.50)),\n",
    "        \"p95\": float(x.quantile(0.95)),\n",
    "        \"p99\": float(x.quantile(0.99)),\n",
    "        \"cv\": float(x.std(ddof=1) / x.mean()) if x.mean() != 0 and x.shape[0] > 1 else np.nan,\n",
    "    }\n",
    "\n",
    "def plot_timeseries(evt: str, smooth_ms: int = SMOOTH_MS_FAST, ylabel: str = \"count per 1ms bucket\"):\n",
    "    if evt not in n_perf.columns or evt not in e_perf.columns:\n",
    "        print(f\"Missing event: {evt}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    n_sm = rolling_mean_ms(n_perf[evt], n_perf[\"t_ms\"], smooth_ms)\n",
    "    e_sm = rolling_mean_ms(e_perf[evt], e_perf[\"t_ms\"], smooth_ms)\n",
    "\n",
    "    ax.plot(n_perf[\"t_s_aligned\"], n_sm, label=\"neutral\")\n",
    "    ax.plot(e_perf[\"t_s_aligned\"], e_sm, label=\"emotional\")\n",
    "    shade_windows(ax, n_wins_aligned, alpha=0.08)\n",
    "    ax.set_title(f\"{evt} (smoothed {smooth_ms} ms)\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution(evt: str, bins: int = 80):\n",
    "    if evt not in n_perf.columns or evt not in e_perf.columns:\n",
    "        print(f\"Missing event: {evt}\")\n",
    "        return\n",
    "    n_x = n_perf[evt].dropna().values\n",
    "    e_x = e_perf[evt].dropna().values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.hist(n_x, bins=bins, density=True, alpha=0.5, label=\"neutral\")\n",
    "    ax.hist(e_x, bins=bins, density=True, alpha=0.5, label=\"emotional\")\n",
    "    ax.set_title(f\"{evt} distribution (per 1ms bucket)\")\n",
    "    ax.set_xlabel(\"value\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def summarize_evt(evt: str) -> pd.DataFrame:\n",
    "    if evt not in n_perf.columns or evt not in e_perf.columns:\n",
    "        return pd.DataFrame([{\"event\": evt, \"error\": \"missing\"}])\n",
    "    s = {\n",
    "        \"event\": evt,\n",
    "        **{f\"neutral_{k}\": v for k,v in describe_series(n_perf[evt]).items()},\n",
    "        **{f\"emotional_{k}\": v for k,v in describe_series(e_perf[evt]).items()},\n",
    "    }\n",
    "    return pd.DataFrame([s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f256e",
   "metadata": {},
   "source": [
    "## Interrupt activity (IRQ / softIRQ)\n",
    "\n",
    "These reflect asynchronous kernel activity. We compare smoothed time series and per-bucket distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b836e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "irq_events = [\n",
    "    \"irq:irq_handler_entry\",\n",
    "    \"irq:softirq_raise\",\n",
    "    \"irq:softirq_entry\",\n",
    "    \"irq:softirq_exit\",\n",
    "    \"irq:tasklet_entry\",\n",
    "    \"irq:tasklet_exit\",\n",
    "]\n",
    "irq_events = [e for e in irq_events if e in common_events]\n",
    "irq_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for evt in irq_events:\n",
    "    plot_timeseries(evt, smooth_ms=SMOOTH_MS_FAST)\n",
    "    plot_distribution(evt, bins=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bac926",
   "metadata": {},
   "source": [
    "## TLB flush activity (proxy for shootdowns)\n",
    "\n",
    "We plot the time series and an inter-arrival-time (IAT) distribution derived from bucketed counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399059cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TLB_EVT = \"tlb:tlb_flush\"\n",
    "assert TLB_EVT in common_events, \"tlb:tlb_flush not available in both runs\"\n",
    "\n",
    "plot_timeseries(TLB_EVT, smooth_ms=SMOOTH_MS_FAST)\n",
    "plot_distribution(TLB_EVT, bins=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccedeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_counts_to_event_times(t_s: np.ndarray, counts: np.ndarray) -> np.ndarray:\n",
    "    # Expand per-bucket counts into approximate event times at bucket centers.\n",
    "    # This is an approximation but sufficient for comparing IAT distributions.\n",
    "    times = []\n",
    "    for ts, c in zip(t_s, counts):\n",
    "        if np.isnan(c) or c <= 0:\n",
    "            continue\n",
    "        k = int(round(c))\n",
    "        times.extend([ts] * k)\n",
    "    return np.array(times, dtype=float)\n",
    "\n",
    "def plot_iat(evt: str, max_ms: int = 200, bins: int = 120):\n",
    "    n_times = bucket_counts_to_event_times(n_perf[\"t_s_aligned\"].values, n_perf[evt].values)\n",
    "    e_times = bucket_counts_to_event_times(e_perf[\"t_s_aligned\"].values, e_perf[evt].values)\n",
    "\n",
    "    n_iat = np.diff(np.sort(n_times)) * 1000.0  # ms\n",
    "    e_iat = np.diff(np.sort(e_times)) * 1000.0  # ms\n",
    "\n",
    "    n_iat = n_iat[(n_iat >= 0) & (n_iat <= max_ms)]\n",
    "    e_iat = e_iat[(e_iat >= 0) & (e_iat <= max_ms)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.hist(n_iat, bins=bins, density=True, alpha=0.5, label=\"neutral\")\n",
    "    ax.hist(e_iat, bins=bins, density=True, alpha=0.5, label=\"emotional\")\n",
    "    ax.set_title(f\"Inter-arrival times for {evt} (clipped to {max_ms} ms)\")\n",
    "    ax.set_xlabel(\"IAT (ms)\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Neutral IAT stats:\", describe_series(pd.Series(n_iat)))\n",
    "    print(\"Emotional IAT stats:\", describe_series(pd.Series(e_iat)))\n",
    "\n",
    "plot_iat(TLB_EVT, max_ms=200, bins=120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31703aa",
   "metadata": {},
   "source": [
    "## PMU jitter proxies\n",
    "\n",
    "We compute IPC (instructions per cycle) per 1ms bucket as a simple microarchitectural stability proxy.\n",
    "We compare time series (smoothed) and distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cycles and instructions columns (names differ)\n",
    "cycles_col = next((c for c in [\"cycles\", \"cpu-cycles\"] if c in n_perf.columns and c in e_perf.columns), None)\n",
    "inst_col   = next((c for c in [\"instructions\"] if c in n_perf.columns and c in e_perf.columns), None)\n",
    "\n",
    "print(\"cycles_col:\", cycles_col, \"inst_col:\", inst_col)\n",
    "\n",
    "assert cycles_col and inst_col, \"Need cycles and instructions in perf output.\"\n",
    "\n",
    "n_ipc = n_perf[inst_col] / n_perf[cycles_col]\n",
    "e_ipc = e_perf[inst_col] / e_perf[cycles_col]\n",
    "\n",
    "n_perf[\"ipc\"] = n_ipc.replace([np.inf, -np.inf], np.nan)\n",
    "e_perf[\"ipc\"] = e_ipc.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "plot_timeseries(\"ipc\", smooth_ms=SMOOTH_MS_FAST, ylabel=\"IPC\")\n",
    "plot_distribution(\"ipc\", bins=100)\n",
    "\n",
    "print(\"IPC stats neutral:\", describe_series(n_perf[\"ipc\"]))\n",
    "print(\"IPC stats emotional:\", describe_series(e_perf[\"ipc\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f397a",
   "metadata": {},
   "source": [
    "## Power / thermal / throttling\n",
    "\n",
    "Energy counters are treated as counters; we take first differences and then smooth heavily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203179b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_rate(series: pd.Series) -> pd.Series:\n",
    "    # perf -I can report per-interval counts or cumulative depending on event; for energy it is often cumulative.\n",
    "    # Taking diff is safe for comparing shape; negative diffs become NaN.\n",
    "    d = series.diff()\n",
    "    d[d < 0] = np.nan\n",
    "    return d\n",
    "\n",
    "energy_events = [c for c in common_events if \"energy\" in c]\n",
    "throttle_evt = \"core_power.throttle\" if \"core_power.throttle\" in common_events else None\n",
    "thermal_evt = \"msr/cpu_thermal_margin/\" if \"msr/cpu_thermal_margin/\" in common_events else None\n",
    "\n",
    "energy_events, throttle_evt, thermal_evt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06630be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy plots\n",
    "for evt in energy_events:\n",
    "    n_perf[f\"{evt}__d\"] = energy_rate(n_perf[evt])\n",
    "    e_perf[f\"{evt}__d\"] = energy_rate(e_perf[evt])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    n_sm = rolling_mean_ms(n_perf[f\"{evt}__d\"], n_perf[\"t_ms\"], SMOOTH_MS_SLOW)\n",
    "    e_sm = rolling_mean_ms(e_perf[f\"{evt}__d\"], e_perf[\"t_ms\"], SMOOTH_MS_SLOW)\n",
    "    ax.plot(n_perf[\"t_s_aligned\"], n_sm, label=\"neutral\")\n",
    "    ax.plot(e_perf[\"t_s_aligned\"], e_sm, label=\"emotional\")\n",
    "    shade_windows(ax, n_wins_aligned, alpha=0.08)\n",
    "    ax.set_title(f\"{evt} delta per 1ms bucket (smoothed {SMOOTH_MS_SLOW} ms)\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"delta\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# Throttle\n",
    "if throttle_evt:\n",
    "    plot_timeseries(throttle_evt, smooth_ms=SMOOTH_MS_SLOW)\n",
    "    plot_distribution(throttle_evt, bins=80)\n",
    "\n",
    "# Thermal margin (slow)\n",
    "if thermal_evt:\n",
    "    plot_timeseries(thermal_evt, smooth_ms=SMOOTH_MS_SLOW, ylabel=\"thermal margin (units as reported)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6682f35",
   "metadata": {},
   "source": [
    "## Kernel log scan (MCE, hardware errors, throttling messages)\n",
    "\n",
    "We extract lines matching key patterns. If empty, that is a valid outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_kernel_log(path: Path, patterns: str = r\"mce|machine check|hardware error|edac|thermal|throttle|watchdog\") -> pd.DataFrame:\n",
    "    rx = re.compile(patterns, flags=re.IGNORECASE)\n",
    "    lines = path.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines()\n",
    "    hits = [ln for ln in lines if rx.search(ln)]\n",
    "    return pd.DataFrame({\"matches\": hits})\n",
    "\n",
    "n_hits = scan_kernel_log(n_dir / \"kernel_log.txt\")\n",
    "e_hits = scan_kernel_log(e_dir / \"kernel_log.txt\")\n",
    "\n",
    "print(\"Neutral kernel matches:\", len(n_hits))\n",
    "print(\"Emotional kernel matches:\", len(e_hits))\n",
    "\n",
    "display(n_hits.head(20))\n",
    "display(e_hits.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e2d98",
   "metadata": {},
   "source": [
    "## Summary table\n",
    "\n",
    "This produces a compact comparison table for the signals you care about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "\n",
    "# Interrupts + TLB\n",
    "for evt in (irq_events + [TLB_EVT]):\n",
    "    if evt in n_perf.columns and evt in e_perf.columns:\n",
    "        summary_rows.append(summarize_evt(evt).iloc[0].to_dict())\n",
    "\n",
    "# IPC\n",
    "summary_rows.append(summarize_evt(\"ipc\").iloc[0].to_dict())\n",
    "\n",
    "# Throttle\n",
    "if throttle_evt:\n",
    "    summary_rows.append(summarize_evt(throttle_evt).iloc[0].to_dict())\n",
    "\n",
    "# Energy deltas (use derived columns)\n",
    "for evt in energy_events:\n",
    "    col = f\"{evt}__d\"\n",
    "    if col in n_perf.columns and col in e_perf.columns:\n",
    "        # temporarily create standard names for summarizer\n",
    "        n_perf[col] = n_perf[col]\n",
    "        e_perf[col] = e_perf[col]\n",
    "        # manual describe\n",
    "        s = {\"event\": col}\n",
    "        s.update({f\"neutral_{k}\": v for k,v in describe_series(n_perf[col]).items()})\n",
    "        s.update({f\"emotional_{k}\": v for k,v in describe_series(e_perf[col]).items()})\n",
    "        summary_rows.append(s)\n",
    "\n",
    "summary = pd.DataFrame(summary_rows)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: export all current figures to disk (uncomment if desired)\n",
    "# out_fig = Path.cwd() / \"figures\" / f\"{NEUTRAL_RUN}__vs__{EMOTIONAL_RUN}\"\n",
    "# out_fig.mkdir(parents=True, exist_ok=True)\n",
    "# for i, fig_num in enumerate(plt.get_fignums(), start=1):\n",
    "#     fig = plt.figure(fig_num)\n",
    "#     fig.savefig(out_fig / f\"fig_{i:02d}.png\", bbox_inches=\"tight\")\n",
    "# print(\"Saved to:\", out_fig)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
